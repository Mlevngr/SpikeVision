{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: z.shen1@tue.nl, f.corradi@tue.nl\n",
    "# Training SpikeVision for DVS 128 dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from timm.scheduler.step_lr import StepLRScheduler\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanbo/aienv2024/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/zhanbo/aienv2024/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folder_src = \"./results/\"\n",
    "try:\n",
    "    os.mkdir(folder_src)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dataset = \"DVS128\"\n",
    "folder = folder_src + f\"{dataset}/\"\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from SV import SpikeVision\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from torch.amp import GradScaler, autocast\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "layers = 2\n",
    "in_channels = 2\n",
    "train_threshold = False\n",
    "image_size = 128\n",
    "dataset_classes = 11\n",
    "num_epochs = 200\n",
    "time_steps = 8\n",
    "embed_dim = 256\n",
    "threshold = [128/128, 128/128, 128/128]\n",
    "pooling_state = \"1111\"\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "precision_epochs = num_epochs\n",
    "precision_bits = 8\n",
    "load = False\n",
    "train_loss_fn, test_loss_fn = criteria, criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating\n",
    "event_per_map = 15000\n",
    "def integrate_fixed_events(events, H, W, events_per_map = event_per_map):\n",
    "    t, x, y, p = (events[key] for key in ('t', 'x', 'y', 'p'))\n",
    "    total_events = len(t)\n",
    "    num_maps = int(np.ceil(total_events / events_per_map))\n",
    "    frames = np.zeros([num_maps, 2, H, W], dtype=np.float32)\n",
    "\n",
    "    for i in range(num_maps):\n",
    "        start_index = i * events_per_map\n",
    "        end_index = min((i + 1) * events_per_map, total_events)\n",
    "        for j in range(start_index, end_index):\n",
    "            if p[j] == 1:\n",
    "                frames[i, 1, y[j], x[j]] += 1\n",
    "            else:\n",
    "                frames[i, 0, y[j], x[j]] += 1\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../data/DVS128/integrate_fixed_events] already exists.\n",
      "The directory [../data/DVS128/integrate_fixed_events] already exists.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = DVS128Gesture(\n",
    "    \"../data/DVS128/\",\n",
    "    train=True,\n",
    "    data_type=\"frame\",\n",
    "    custom_integrate_function=integrate_fixed_events\n",
    ")\n",
    "dataset_test = DVS128Gesture(\n",
    "    \"../data/DVS128/\",\n",
    "    train=False,\n",
    "    data_type=\"frame\", \n",
    "    custom_integrate_function=integrate_fixed_events\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "test_size = len(combined_dataset) - train_size\n",
    "generator = torch.Generator()\n",
    "seed = np.random.randint(low=0, high=100)\n",
    "print(seed)\n",
    "generator.manual_seed(seed)\n",
    "dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "def custom_collate_fn(batch):\n",
    "    max_timesteps = time_steps\n",
    "    padded_batch = []\n",
    "\n",
    "    for item in batch:\n",
    "        data, label = item\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data).float()\n",
    "\n",
    "        current_timesteps = data.size(0)\n",
    "\n",
    "        if current_timesteps < max_timesteps:\n",
    "            padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "            padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "        else:\n",
    "            padded_data = data[:max_timesteps]\n",
    "\n",
    "        padded_batch.append((padded_data, label))\n",
    "\n",
    "    return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = SpikeVision(\n",
    "    dataset=dataset,\n",
    "    image_size_h=image_size,\n",
    "    image_size_w=image_size,\n",
    "    input_channels=in_channels,\n",
    "    num_classes=dataset_classes,\n",
    "    embed_dims=embed_dim,\n",
    "    threshold_head=threshold[0],\n",
    "    threshold_conv=threshold[1],\n",
    "    threshold_scre=threshold[2],\n",
    "    depths=layers,\n",
    "    pooling_state=pooling_state,\n",
    "    train_threshold=train_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_eval_loss_list = []\n",
    "test_loss_list = []\n",
    "def train(model, train_loader, test_loader, optimizer, scheduler=None, num_epochs=50, precision_epochs=num_epochs, low_precision=8, load=False, path=None):\n",
    "    if load:\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    acc_state = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        predictions = []\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f\"{len(train_loader)} batches in one epoch.\")\n",
    "        \n",
    "        for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "\n",
    "            with autocast(device_type=f\"{device}\"):\n",
    "                outputs = model(images)\n",
    "                loss = train_loss_fn(outputs, labels)\n",
    "                prediction = outputs.argmax(axis=1)\n",
    "            scaler.scale(loss).backward()\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "            train_acc += (prediction == labels).sum().item()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        if (epoch >= precision_epochs):\n",
    "            low_precision_state = low_precision(model.state_dict(), precision=low_precision)\n",
    "            if epoch == precision_epochs:\n",
    "                torch.save(model.state_dict(), path)\n",
    "                path = folder + \"low_precision_model\" + path\n",
    "                for param in optimizer.param_groups:\n",
    "                    param['lr'] = 1e-3\n",
    "            model.load_state_dict(low_precision_state)\n",
    "            model.to(device)\n",
    "        \n",
    "        test_acc, train_eval_acc, test_loss = test(model, test_loader, train_loader)\n",
    "        if test_acc >= acc_state:\n",
    "            acc_state = test_acc\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(\"Checkpoint saved.\")\n",
    "        \n",
    "        train_loss = train_loss_sum / len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        scheduler.step(epoch)\n",
    "\n",
    "        print(f\"Highest test accuracy: {acc_state}\")\n",
    "        print(f\"Epoch: {epoch:3d}, Train loss: {train_loss:.4f}, Train accuracy: {train_acc / len(train_loader.dataset):.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.eval()\n",
    "        if len(images.shape) == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        loss = test_loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        prediction = outputs.argmax(axis=1)\n",
    "        total_correct += (prediction == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = total_correct / len(loader.dataset)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def test(model, test_loader, train_loader):\n",
    "    model.eval()\n",
    "    test_acc, test_loss = evaluate(model, test_loader)\n",
    "    train_eval_acc, train_eval_loss = evaluate(model, train_loader)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Train eval Loss: {train_eval_loss:.4f}, Train eval Acc: {train_eval_acc:.4f}')\n",
    "    return test_acc, train_eval_acc, test_loss\n",
    "\n",
    "def precision_transfer(x, precision_bits=8, threshold=1.0, max_value=0.5, min_value=-0.5):\n",
    "    x_flat = x.flatten()\n",
    "    step = np.diff(np.linspace(min_value, max_value, num=2**precision_bits)[0:2])\n",
    "    max_value = max_value - step[0]\n",
    "    q_list = np.round(np.linspace(min_value, max_value, num=2**precision_bits), precision_bits - 2)\n",
    "\n",
    "    func = lambda x: q_list[np.abs(q_list - x).argmin()]\n",
    "    q_list = np.array(list(map(func, x_flat)))\n",
    "    q_list = q_list.reshape(x.shape)\n",
    "    return q_list\n",
    "\n",
    "def low_precision(state_dict, precision=8, threshold=1.0, max_value=0.5, min_value=-0.5):\n",
    "    for key in state_dict.keys():\n",
    "        if \"threshold\" in key:\n",
    "            continue\n",
    "        state_dict[key] = precision_transfer(state_dict[key].cpu().numpy(), precision_bits=precision, threshold=threshold)\n",
    "\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler(args, optimizer):\n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    if getattr(args, 'lr_noise', None) is not None:\n",
    "        lr_noise = getattr(args, 'lr_noise')\n",
    "        if isinstance(lr_noise, (list, tuple)):\n",
    "            noise_range = [n * num_epochs for n in lr_noise]\n",
    "            if len(noise_range) == 1:\n",
    "                noise_range = noise_range[0]\n",
    "        else:\n",
    "            noise_range = lr_noise * num_epochs\n",
    "    else:\n",
    "        noise_range = None\n",
    "    noise_args = dict(\n",
    "        noise_range_t=noise_range,\n",
    "        noise_pct=getattr(args, 'lr_noise_pct', 0.67),\n",
    "        noise_std=getattr(args, 'lr_noise_std', 1.),\n",
    "        noise_seed=getattr(args, 'seed', 42),\n",
    "    )\n",
    "    cycle_args = dict(\n",
    "        cycle_mul=getattr(args, 'lr_cycle_mul', 1.),\n",
    "        cycle_decay=getattr(args, 'lr_cycle_decay', 0.1),\n",
    "        cycle_limit=getattr(args, 'lr_cycle_limit', 1),\n",
    "    )\n",
    "    lr_scheduler = StepLRScheduler(\n",
    "        optimizer,\n",
    "        decay_t=args.decay_epochs,\n",
    "        decay_rate=args.decay_rate,\n",
    "        warmup_lr_init=args.warmup_lr,\n",
    "        warmup_t=args.warmup_epochs,\n",
    "        **noise_args,\n",
    "    )\n",
    "\n",
    "\n",
    "    return lr_scheduler, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "learning_rate = 1e-3\n",
    "path = folder + f\"model_{dataset}_embeddim_{embed_dim}_depth_{layers}_{time}.pth\"\n",
    "cooldown = 10\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epochs = num_epochs\n",
    "        self.sched = 'step'  \n",
    "        self.min_lr = 1e-5\n",
    "        self.warmup_lr = 3e-4\n",
    "        self.warmup_epochs = 20\n",
    "        self.decay_rate = 0.9\n",
    "        self.cooldown_epochs = cooldown\n",
    "        self.lr_noise = [0.6, 0.9]\n",
    "        self.lr_noise_pct = 0.67\n",
    "        self.lr_noise_std = 1.0\n",
    "        self.seed = 42\n",
    "        self.decay_epochs = 20\n",
    "        self.patience_epochs = 5\n",
    "\n",
    "args = Args()\n",
    "\n",
    "optimizer = torch.optim.Adam([{ 'params': base_params}], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "scheduler, num_epochs = create_scheduler(args, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zhanbo/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the key with your own key\n",
    "wandb.login(key='aee84de3d1a27bf9075037431c3793c98d1813a6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
