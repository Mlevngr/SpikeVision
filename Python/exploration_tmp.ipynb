{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "Device in use: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federico/aienv/aienv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/federico/aienv/aienv/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/federico/aienv/aienv/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# author f.corradi@tue.nl z.shen1@student.tue.nl\n",
    "# Training Spiketransformer solving image classification\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "folder_src = \"./result/\"\n",
    "srbm_fpga = ''\n",
    "datasets = [\"MNIST\", \"genomic\", \"DVS128\", \"PSMNIST\", \"NMNIST\", \"cifar10\"]\n",
    "index_dataset = 2\n",
    "dataset = datasets[index_dataset]\n",
    "\n",
    "today = date.today()\n",
    "folder = folder_src + f\"{dataset}/\"\n",
    "try:\n",
    "    os.stat(folder)\n",
    "except:\n",
    "    os.mkdir(folder)\n",
    "\n",
    "from model import *\n",
    "# from model import sdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from contextlib import suppress\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from pytorch_lamb import Lamb\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "from torch import Generator\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, CosineAnnealingLR\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, Subset, ConcatDataset, random_split\n",
    "from timm.data import create_loader\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from timm.scheduler.multistep_lr import MultiStepLRScheduler\n",
    "from timm.scheduler.plateau_lr import PlateauLRScheduler\n",
    "from timm.scheduler.poly_lr import PolyLRScheduler\n",
    "from timm.scheduler.step_lr import StepLRScheduler\n",
    "from timm.scheduler.tanh_lr import TanhLRScheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "import dvs_utils\n",
    "from memory_profiler import profile\n",
    "from module.lif_neuron import lif_neuron, LIFNeuron\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from pylab import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2460109/2736003858.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.clock_driven import functional\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from spikingjelly.datasets.n_mnist import NMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "genomic_folders = ['D1-mild-bottleneck/', 'D2-severe-bottleneck/', 'D3-recent-migration/', 'D4-old-migration/', 'D5-low-intensity-recombination/', 'D6-high-intensity-recombination/', 'D7-Recombination-hotspot-intensity2/', 'D8-Recombination-hotspot-intensity10/']\n",
    "data_index = 0\n",
    "batch_size = 20\n",
    "num_workers = 0\n",
    "num_heads = 1\n",
    "mlp_ratio = 2\n",
    "layers = 2\n",
    "in_channels = 1\n",
    "spike_mode = \"if\"\n",
    "input_extend = 1\n",
    "dvs_mode = False\n",
    "model_type=\"tsdt\"\n",
    "spsmodule=\"bsps\"\n",
    "encodermodule=\"encoder\"\n",
    "train_threshold = False\n",
    "if dataset == \"MNIST\":\n",
    "    in_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {\"MNIST\" : 28, \"DVS128\" : 128, \"genomic\" : 128, \"PSMNIST\" : 98, \"cifar10\":64, \"NMNIST\":34, \"cifar10\":128}\n",
    "dataset_classes = {\"MNIST\" : 10, \"DVS128\" : 11, \"genomic\" : 2, \"PSMNIST\" : 10, \"cifar10\":10, \"NMNIST\":10, \"cifar10\":10}\n",
    "\n",
    "image_size = dataset_size[dataset]\n",
    "num_classes = dataset_classes[dataset]\n",
    "num_epoch = 50\n",
    "drop_rate = 0\n",
    "\n",
    "if dataset == \"DVS128\":\n",
    "    time_steps = 16\n",
    "    embed_dims = 256\n",
    "    patch_size = 8\n",
    "    input_extend = 1\n",
    "    num_epoch = 200\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "    pooling_stat = \"1111\"\n",
    "elif dataset == \"NMNIST\":\n",
    "    time_steps = 4\n",
    "    embed_dims = 64\n",
    "    patch_size = 8\n",
    "    batch_size = 500\n",
    "    input_extend = 1\n",
    "    num_epoch = 100\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "    pooling_stat = \"0011\"\n",
    "elif dataset == \"PSMNIST\":\n",
    "    time_steps = 4\n",
    "    embed_dims = 64\n",
    "    batch_size = 500\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "    patch_size = 2\n",
    "    pooling_stat = \"0011\"\n",
    "elif dataset == \"genomic\":\n",
    "    num_epoch = 200\n",
    "    batch_size = 200\n",
    "    time_steps = 2\n",
    "    embed_dims = 64\n",
    "    patch_size = 16\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "    pooling_stat = \"1111\"\n",
    "elif dataset == \"cifar10\":\n",
    "    time_steps = 16\n",
    "    embed_dims = 256\n",
    "    patch_size = 4\n",
    "    batch_size = 20\n",
    "    num_epoch = 150\n",
    "    pooling_stat = \"1111\"\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "else:\n",
    "    time_steps = 8\n",
    "    embed_dims = 64\n",
    "    patch_size = 4\n",
    "    batch_size = 500\n",
    "    thresholds = [128/128, 128/128, 128/128]\n",
    "    pooling_stat = \"0011\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikingjelly.datasets as sjds\n",
    "events_per_frame = 15000\n",
    "events_per_frame1 = 10000\n",
    "def integrate_fixed_15000_events(events, H, W, events_per_frame=events_per_frame):\n",
    "    t, x, y, p = (events[key] for key in ('t', 'x', 'y', 'p'))\n",
    "    total_events = len(t)\n",
    "    num_frames = int(np.ceil(total_events / events_per_frame))\n",
    "    frames = np.zeros([num_frames, 2, H, W], dtype=np.float32)\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_index = i * events_per_frame\n",
    "        end_index = min((i + 1) * events_per_frame, total_events)\n",
    "        for j in range(start_index, end_index):\n",
    "            if p[j] == 1:\n",
    "                frames[i, 1, y[j], x[j]] += 1\n",
    "            else:\n",
    "                frames[i, 0, y[j], x[j]] += 1\n",
    "\n",
    "    return frames\n",
    "\n",
    "def integrate_fixed_10000_events(events, H, W, events_per_frame=events_per_frame):\n",
    "    t, x, y, p = (events[key] for key in ('t', 'x', 'y', 'p'))\n",
    "    total_events = len(t)\n",
    "    num_frames = int(np.ceil(total_events / events_per_frame1))\n",
    "    frames = np.zeros([num_frames, 2, H, W], dtype=np.float32)\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start_index = i * events_per_frame1\n",
    "        end_index = min((i + 1) * events_per_frame1, total_events)\n",
    "        for j in range(start_index, end_index):\n",
    "            if p[j] == 1:\n",
    "                frames[i, 1, y[j], x[j]] += 1\n",
    "            else:\n",
    "                frames[i, 0, y[j], x[j]] += 1\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DVS128'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n",
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n"
     ]
    }
   ],
   "source": [
    "if dataset == \"DVS128\":\n",
    "#     dataset_train = DVS128Gesture(\n",
    "#         \"data/DVS128/\",\n",
    "#         train=True,\n",
    "#         data_type=\"frame\",\n",
    "#         frames_number=time_steps,\n",
    "#         split_by=\"number\",\n",
    "#     )\n",
    "#     dataset_test = DVS128Gesture(\n",
    "#         \"data/DVS128/\",\n",
    "#         train=False,\n",
    "#         data_type=\"frame\",\n",
    "#         frames_number=time_steps,\n",
    "#         split_by=\"number\",\n",
    "#     )\n",
    "    dataset_train = DVS128Gesture(\n",
    "        \"data/DVS128/\",\n",
    "        train=True,\n",
    "        data_type=\"frame\",\n",
    "        custom_integrate_function=integrate_fixed_15000_events\n",
    "    )\n",
    "    dataset_test = DVS128Gesture(\n",
    "        \"data/DVS128/\",\n",
    "        train=False,\n",
    "        data_type=\"frame\", \n",
    "        custom_integrate_function=integrate_fixed_15000_events\n",
    "    )\n",
    "elif dataset == \"MNIST\":\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset_train = MNIST(\n",
    "        \"data/mnist/\",\n",
    "        train = True,\n",
    "        transform=transform\n",
    "    )\n",
    "    dataset_test = MNIST(\n",
    "        \"data/mnist/\",\n",
    "        train = False,\n",
    "        transform=transform\n",
    "    )\n",
    "elif dataset == \"cifar10\":\n",
    "    dataset_tmp = CIFAR10DVS(\n",
    "        \"../snntransformer/data/cifar10-dvs/\",\n",
    "        data_type=\"frame\",\n",
    "        custom_integrate_function=integrate_fixed_15000_events\n",
    "    )\n",
    "    dataset_train, dataset_test = dvs_utils.split_to_train_test_set(\n",
    "        0.8, dataset_tmp, 42\n",
    "    )\n",
    "elif dataset == \"genomic\":\n",
    "    dataset_train = \"../biosnn-master/data/Row&column/\" + genomic_folders[data_index] + \"train/images/\"\n",
    "    dataset_test = \"../biosnn-master/data/Row&column/\" + genomic_folders[data_index] + \"test/images/\"\n",
    "else:\n",
    "    class ReshapeTransform:\n",
    "        def __init__(self, new_size):\n",
    "            self.new_size = new_size\n",
    "\n",
    "        def __call__(self, img):\n",
    "            return img.view(self.new_size)\n",
    "        \n",
    "    class PixelPermuteTransform:\n",
    "        def __init__(self, perm):\n",
    "            self.perm = perm\n",
    "\n",
    "        def __call__(self, img):\n",
    "            img = img[:,self.perm, :]\n",
    "            return img\n",
    "    np.random.seed(42)\n",
    "    rng = np.random.default_rng()\n",
    "    perm = rng.permutation(image_size)\n",
    "    print(perm)\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,)),\n",
    "    ReshapeTransform((-1, image_size, 28*28//image_size)),\n",
    "    PixelPermuteTransform(perm),\n",
    "    ])\n",
    "    dataset_train = MNIST(\n",
    "        \"data/mnist/\",\n",
    "        train=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    dataset_test = MNIST(\n",
    "        \"data/mnist/\",\n",
    "        train=False,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "loader_train, loader_eval, train_idx = None, None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "if dataset != \"cifar10\":\n",
    "    combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "    train_size = int(0.8 * len(combined_dataset))\n",
    "    test_size = len(combined_dataset) - train_size\n",
    "    generator = torch.Generator()\n",
    "    seed = np.random.randint(low=0, high=100)\n",
    "    print(seed)\n",
    "    generator.manual_seed(43)\n",
    "    dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "Create dataloader: DVS128\n"
     ]
    }
   ],
   "source": [
    "if dataset == \"DVS128\" or dataset == \"SHD\" or dataset == \"cifar10\" or dataset == \"NMNIST\":\n",
    "#     loader_train = DataLoader(\n",
    "#         dataset_train,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "#     loader_test = DataLoader(\n",
    "#         dataset_test,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "#     def custom_collate_fn(batch):\n",
    "#         batch = [(item[0][:4, :, :, :], item[1]) for item in batch]\n",
    "#         return torch.utils.data.dataloader.default_collate(batch)\n",
    "    def custom_collate_fn(batch):\n",
    "        max_timesteps = time_steps\n",
    "        padded_batch = []\n",
    "\n",
    "        for item in batch:\n",
    "            data, label = item\n",
    "            if isinstance(data, np.ndarray):\n",
    "                data = torch.from_numpy(data).float()\n",
    "\n",
    "            current_timesteps = data.size(0)\n",
    "\n",
    "            if current_timesteps < max_timesteps:\n",
    "                padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "                padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "            else:\n",
    "                padded_data = data[:max_timesteps]\n",
    "\n",
    "            padded_batch.append((padded_data, label))\n",
    "\n",
    "        return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    print(len(loader_train))\n",
    "elif dataset == \"genomic\": \n",
    "    transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                   transforms.RandomCrop((128, 128,), padding=None, \n",
    "                                                         pad_if_needed=False, \n",
    "                                                         fill=0, \n",
    "                                                         padding_mode='constant'),\n",
    "                                   transforms.ToTensor()])\n",
    "    train_data = torchvision.datasets.ImageFolder(root=dataset_train, transform=transform)\n",
    "    test_data = torchvision.datasets.ImageFolder(root=dataset_test, transform=transform)\n",
    "    loader_train = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "else:\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "print(\"Create dataloader: {}\".format(dataset)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVS128\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(102.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20, 2, 128, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(loader_train)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "# print(example_data.shape)\n",
    "if dataset == \"DVS128\" or dataset == \"cifar10\":\n",
    "    example_data = example_data.transpose(0, 1)\n",
    "elif dataset == \"SHD\":\n",
    "    time_steps=len(example_data[0])\n",
    "print(torch.max(example_data))\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 2 if dataset == \"DVS128\" or dataset == \"NMNIST\" or dataset == \"cifar10\" else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 20, 2, 128, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if dataset == \"PSMNIST\":\n",
    "    image_size_h = image_size \n",
    "    image_size_w = 28*28//image_size\n",
    "else:\n",
    "    example_data = example_data.cpu().detach().numpy()\n",
    "    image_size_h = image_size\n",
    "    image_size_w = image_size\n",
    "\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset != \"DVS128\" and dataset != \"NMNIST\" and dataset != \"cifar10\":\n",
    "    fig, idx = plt.subplots(2, 2)\n",
    "    subfigs = []\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            subfigs.append(idx[i, j])\n",
    "\n",
    "    for i in range(4):\n",
    "        subfigs[i].imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "        subfigs[i].set_title('{}'.format(type[example_targets[i]]))\n",
    "        subfigs[i].set_xticks([])\n",
    "        subfigs[i].set_yticks([])\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_fn = criterion.cuda()\n",
    "test_loss_fn = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model_type==\"bsdt\":\n",
    "    model = bsdt(\n",
    "        dataset=dataset,\n",
    "        spsmodule=spsmodule,\n",
    "        encodermodule=encodermodule,\n",
    "        T=time_steps,\n",
    "        img_size_h=image_size, \n",
    "        img_size_w=image_size_w,\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=patch_size,\n",
    "        embed_dims=embed_dims,\n",
    "        num_heads=num_heads,\n",
    "        pooling_stat=pooling_stat,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        threshold_head=thresholds[0],\n",
    "        threshold_sps=thresholds[1],\n",
    "        threshold_enc=thresholds[2],\n",
    "        depths=layers,\n",
    "        spike_mode=spike_mode,\n",
    "        dvs_mode=dvs_mode,\n",
    "        train_threshold=train_threshold,\n",
    "    )\n",
    "elif model_type==\"tsdt\":\n",
    "    model = tsdt(\n",
    "        dataset=dataset,\n",
    "        spsmodule=spsmodule,\n",
    "        encodermodule=encodermodule,\n",
    "        T=time_steps,\n",
    "        img_size_h=image_size, \n",
    "        img_size_w=image_size_w,\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=patch_size,\n",
    "        embed_dims=embed_dims,\n",
    "        num_heads=num_heads,\n",
    "        pooling_stat=pooling_stat,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        threshold_head=thresholds[0],\n",
    "        threshold_sps=thresholds[1],\n",
    "        threshold_enc=thresholds[2],\n",
    "        depths=layers,\n",
    "        train_threshold=train_threshold,\n",
    "    )\n",
    "image_size_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(\"train\")\n",
    "stream_handler = logging.StreamHandler()\n",
    "format_str = \"%(asctime)s %(levelname)s: %(message)s\"\n",
    "stream_handler.setFormatter(logging.Formatter(format_str))\n",
    "_logger.addHandler(stream_handler)\n",
    "_logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_eval_loss_list = []\n",
    "test_loss_list = []\n",
    "def train(model, loader, test_loader, optimizer, scheduler=None, num_epochs=50, chan=1, precision_epochs=40, load=False, path=None):\n",
    "    best_acc = 0\n",
    "#     clipper = bsdt.Clipper()\n",
    "    \n",
    "    if load:\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        \n",
    "    acc_state = 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_acc = 0.0\n",
    "        train_loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        predictions = []\n",
    "        if epoch == 0:\n",
    "            print(len(loader))\n",
    "        \n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "#             if i >= 80:\n",
    "#                 break\n",
    "            labels = labels#.clone().view((-1, labels.shape[0]))\n",
    "            \n",
    "            if len(images.shape) == 3:\n",
    "                images.unsqueeze_(dim=1)\n",
    "                \n",
    "            model.train()\n",
    "            \n",
    "            with autocast():\n",
    "                x, y, z = model(images)\n",
    "\n",
    "                train_loss = criterion(x.to(device), labels.to(device))\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "                pred_ = x.argmax(axis=1)\n",
    "                predictions.append(pred_.data.cpu().numpy())\n",
    "            scaler.scale(train_loss).backward()\n",
    "            \n",
    "            # gradient clamp\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                max_norm=1000,\n",
    "                norm_type=2,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            train_loss_sum += train_loss.item()\n",
    "            predicted = pred_.cpu()\n",
    "            if i == 0:\n",
    "                print('\\n', 'Starting Epoch:', epoch)\n",
    "                print(model.lif_y_1.threshold.data)\n",
    "#                 print(y[0, 0:3, :], '\\n')\n",
    "#                 print(y[0, 0:3, :], '\\n')\n",
    "                print(model.patch_embed.conv1.weight[0][0])\n",
    "#                 print(z[0, 0:3, 0, :])\n",
    "#                 print(z[0:3, :].max())\n",
    "#                 print(' ', labels, '\\n', pred_)\n",
    "                # for module in model.modules():\n",
    "                #     if isinstance(module, LIFNeuron):\n",
    "                #         print(module, module.threshold.grad)\n",
    "#                 print(\"Threshold grad:\", model.patch_embed.lif_infor.threshold.grad)\n",
    "#                 print(\"Threshold value:\", model.patch_embed.lif_infor.threshold.data)\n",
    "            temp_acc = (predicted == labels.cpu()).sum()\n",
    "            train_acc += (predicted == labels.cpu()).sum()\n",
    "#             if i % 10 == 0:\n",
    "#                 print(f\"Iteration {i}\")\n",
    "#                 print(torch.cuda.memory_summary(device=None, abbreviated=True))\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "#             for module in model.modules():\n",
    "#                 if isinstance(module, LIFNeuron):\n",
    "#                     module.threshold_positive()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'], 1e-10)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        if (epoch >= precision_epochs):\n",
    "            low_precision_state = low_precision(model.state_dict(), precision=n_bits)\n",
    "            high_precision_state = model.state_dict()\n",
    "            if epoch == precision_epochs:\n",
    "                torch.save(model.state_dict(), path)\n",
    "                acc_state = 0\n",
    "                path = folder +\"dataset_\" + str(data_index+1) + \"_timestep_\"+str(time_steps)+\"_head_\"+str(num_heads)+\"_mlp_ratio_\"+str(mlp_ratio)+\"_encoder_\"+str(layers)+\"_embed_dims_\"+str(embed_dims)+\"_nbits_\"+str(n_bits)+\"_ep_\"+str(num_epoch)+\"_lr(e-3)_\"+str(learning_rate * 1000)+ \"_low_precision\"+\".pth\"\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-3\n",
    "                    # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True)\n",
    "#                 scheduler = StepLR(optimizer, step_size=10, gamma=.8)\n",
    "            model.load_state_dict(low_precision_state)\n",
    "            model.to(device)\n",
    "\n",
    "#         train_acc = train_acc.data.cpu().numpy()\n",
    "        test_acc, train_eval_acc, test_loss = test(model, loader, test_loader)\n",
    "        if test_acc > acc_state:\n",
    "            torch.save(model.state_dict(), path)\n",
    "            acc_state = test_acc\n",
    "            print('Checkpoint saved')\n",
    "        train_loss = train_loss_sum/len(loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        # train_eval_loss_list.append(train_eval_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step(train_loss)\n",
    "        scheduler.step(epoch)\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.clamp_(min=-2, max=2)\n",
    "        print(\"Highest test acc: {:.4f}\".format(acc_state))\n",
    "        print(\"epoch: {:3d}, Train Loss: {:.4f}, Train Acc: {:.4f}, Last batch acc: {:.4f} Learning rate: {}\".format(epoch,\n",
    "                                                                           train_loss,\n",
    "                                                                           train_acc/(batch_size)/len(loader),\n",
    "                                                                            temp_acc/batch_size,\n",
    "                                                                            optimizer.param_groups[0]['lr']))\n",
    "#         torch.cuda.empty_cache()\n",
    "def evaluate(model, loader):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if len(images.shape) == 3:\n",
    "            images = images.unsqueeze(dim=1)\n",
    "        outputs, _, _ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * images.size(0)  # 乘以当前批次的大小\n",
    "        pred_ = outputs.argmax(dim=1)\n",
    "        if i == 0:\n",
    "            print(' ', labels, '\\n', pred_)\n",
    "        total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    print(\"Train eval: \")\n",
    "    train_loss, train_acc = evaluate(model, train_loader)\n",
    "    print(\"Test: \")\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "\n",
    "    print(f'Train eval Loss: {train_loss:.4f}, Train eval Acc: {train_acc:.4f}')\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    return test_acc, train_acc, test_loss\n",
    "        \n",
    "# def test(model, train_loader, test_loader, chan=1):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         train_test_accuracy_n = []\n",
    "#         test_accuracy_n = []\n",
    "#         train_test_loss_sum = 0.0\n",
    "#         test_loss_sum = 0.0\n",
    "#         examples_b = []\n",
    "#         for i, (images, labels) in enumerate(train_loader):\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             if len(images.shape) == 3:\n",
    "#                 images.unsqueeze_(dim=1)\n",
    "#             pred_, y, z = model(images)\n",
    "# #             print(pred_.shape)\n",
    "#             train_test_loss = criterion(pred_.to(device), labels.to(device))\n",
    "#             pred_ = pred_.argmax(axis=1)\n",
    "#             train_test_acc = (pred_.to(device) == labels.to(device)).sum().data.cpu().numpy() / float(len(pred_))\n",
    "#             train_test_loss_sum += train_test_loss.detach().cpu().numpy()\n",
    "#             train_test_accuracy_n.append(train_test_acc)\n",
    "        \n",
    "\n",
    "#         for i, (images, labels) in enumerate(test_loader):\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             if len(images.shape) == 3:\n",
    "#                 images.unsqueeze_(dim=1)\n",
    "#             pred_, y, z = model(images)\n",
    "# #             print(pred_.shape)\n",
    "#             test_loss = criterion(pred_.to(device), labels.to(device))\n",
    "#             pred_ = pred_.argmax(axis=1)\n",
    "#             test_acc = (pred_.to(device) == labels.to(device)).sum().data.cpu().numpy() / float(len(pred_))\n",
    "#             test_loss_sum += test_loss.detach().cpu().numpy()\n",
    "#             test_accuracy_n.append(test_acc)\n",
    "#         examples_b.append(images.shape[0])\n",
    "        \n",
    "#         print('Train eval Loss: {:.4f}, Train eval Acc: {:.4f}'.format(np.mean(train_test_loss_sum/len(train_loader)/(batch_size)),\n",
    "#                                                                                np.mean(train_test_accuracy_n)))\n",
    "\n",
    "#         print('Test Loss: {:.4f}, Test Acc: {:.4f}'.format(np.mean(test_loss_sum/len(test_loader)/batch_size),\n",
    "#                                                                                np.mean(test_accuracy_n)))\n",
    "#     return np.mean(test_accuracy_n), np.mean(train_test_loss_sum/len(train_loader)/batch_size), np.mean(test_loss_sum/len(test_loader)/batch_size)\n",
    "\n",
    "def precision_transfer(x, precision_bit=8, threshold=1.0):\n",
    "    if len(x.shape) == 1:\n",
    "        o = x.shape\n",
    "    elif len(x.shape) == 2:\n",
    "        i, o = x.shape\n",
    "    else:\n",
    "        i, o, row, col = x.shape\n",
    "    x_list = x.reshape((-1,))\n",
    "    \n",
    "    A = 8\n",
    "    max_w = threshold/(2**A/2**(precision_bit-1))\n",
    "    min_w = -max_w\n",
    "    step = np.diff(np.linspace(min_w, max_w, 2**precision_bit)[0:2])\n",
    "    max_w = max_w - step[0]\n",
    "    \n",
    "#     n = 2**precision_bit - 1\n",
    "#     qa = (min_w*2.0)/n\n",
    "    q_list = np.round(np.linspace(min_w, max_w, 2**precision_bit), precision_bit - 2)\n",
    "#     q_list[int(2**precision_bit)] = 0\n",
    "    \n",
    "    func = lambda x:q_list[np.abs(x - q_list).argmin()]\n",
    "    q_list = np.array(list(map(func, x_list)))\n",
    "    \n",
    "    if len(x.shape) == 1:\n",
    "        return q_list\n",
    "    elif len(x.shape) == 2:\n",
    "        return q_list.reshape((i, o))\n",
    "    else:\n",
    "        return q_list.reshape((i, o, row, col))\n",
    "    \n",
    "def low_precision(state_dict, precision=8, threshold=1.0):\n",
    "    for k in state_dict.keys():\n",
    "        if not isinstance(k, nn.MaxPool2d) and not isinstance(k, nn.AvgPool2d):\n",
    "            w = precision_transfer(state_dict[k].data.cpu().numpy(), precision_bit=precision, threshold=threshold)\n",
    "            w[state_dict[k].data.cpu().numpy() == 0] = 0\n",
    "            state_dict[k] = torch.tensor(w)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler(args, optimizer):\n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    if getattr(args, 'lr_noise', None) is not None:\n",
    "        lr_noise = getattr(args, 'lr_noise')\n",
    "        if isinstance(lr_noise, (list, tuple)):\n",
    "            noise_range = [n * num_epochs for n in lr_noise]\n",
    "            if len(noise_range) == 1:\n",
    "                noise_range = noise_range[0]\n",
    "        else:\n",
    "            noise_range = lr_noise * num_epochs\n",
    "    else:\n",
    "        noise_range = None\n",
    "    noise_args = dict(\n",
    "        noise_range_t=noise_range,\n",
    "        noise_pct=getattr(args, 'lr_noise_pct', 0.67),\n",
    "        noise_std=getattr(args, 'lr_noise_std', 1.),\n",
    "        noise_seed=getattr(args, 'seed', 42),\n",
    "    )\n",
    "    cycle_args = dict(\n",
    "        cycle_mul=getattr(args, 'lr_cycle_mul', 1.),\n",
    "        cycle_decay=getattr(args, 'lr_cycle_decay', 0.1),\n",
    "        cycle_limit=getattr(args, 'lr_cycle_limit', 1),\n",
    "    )\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if args.sched == 'cosine':\n",
    "        lr_scheduler = CosineLRScheduler(\n",
    "            optimizer,\n",
    "            t_initial=num_epochs,\n",
    "            lr_min=args.min_lr,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            k_decay=getattr(args, 'lr_k_decay', 1.0),\n",
    "            **cycle_args,\n",
    "            **noise_args,\n",
    "        )\n",
    "        num_epochs = lr_scheduler.get_cycle_length() + args.cooldown_epochs\n",
    "    elif args.sched == 'tanh':\n",
    "        lr_scheduler = TanhLRScheduler(\n",
    "            optimizer,\n",
    "            t_initial=num_epochs,\n",
    "            lr_min=args.min_lr,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            t_in_epochs=True,\n",
    "            **cycle_args,\n",
    "            **noise_args,\n",
    "        )\n",
    "        num_epochs = lr_scheduler.get_cycle_length() + args.cooldown_epochs\n",
    "    elif args.sched == 'step':\n",
    "        lr_scheduler = StepLRScheduler(\n",
    "            optimizer,\n",
    "            decay_t=args.decay_epochs,\n",
    "            decay_rate=args.decay_rate,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            **noise_args,\n",
    "        )\n",
    "    elif args.sched == 'multistep':\n",
    "        lr_scheduler = MultiStepLRScheduler(\n",
    "            optimizer,\n",
    "            decay_t=args.decay_milestones,\n",
    "            decay_rate=args.decay_rate,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            **noise_args,\n",
    "        )\n",
    "    elif args.sched == 'plateau':\n",
    "        mode = 'min' if 'loss' in getattr(args, 'eval_metric', '') else 'max'\n",
    "        lr_scheduler = PlateauLRScheduler(\n",
    "            optimizer,\n",
    "            decay_rate=args.decay_rate,\n",
    "            patience_t=args.patience_epochs,\n",
    "            lr_min=args.min_lr,\n",
    "            mode=mode,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            cooldown_t=0,\n",
    "            **noise_args,\n",
    "        )\n",
    "    elif args.sched == 'poly':\n",
    "        lr_scheduler = PolyLRScheduler(\n",
    "            optimizer,\n",
    "            power=args.decay_rate,  # overloading 'decay_rate' as polynomial power\n",
    "            t_initial=num_epochs,\n",
    "            lr_min=args.min_lr,\n",
    "            warmup_lr_init=args.warmup_lr,\n",
    "            warmup_t=args.warmup_epochs,\n",
    "            k_decay=getattr(args, 'lr_k_decay', 1.0),\n",
    "            **cycle_args,\n",
    "            **noise_args,\n",
    "        )\n",
    "        num_epochs = lr_scheduler.get_cycle_length() + args.cooldown_epochs\n",
    "\n",
    "    return lr_scheduler, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "if dataset == 'genomic':\n",
    "    if data_index == 3 or data_index == 4 or data_index == 2 or data_index == 7:\n",
    "        learning_rate = 2e-3\n",
    "    elif data_index == 6:\n",
    "        learning_rate = 2e-3\n",
    "    elif data_index == 5:\n",
    "        learning_rate = 5e-3\n",
    "    else:\n",
    "        learning_rate = 1e-3\n",
    "elif dataset == 'MNIST':\n",
    "    learning_rate = 3e-3\n",
    "elif dataset == \"NMNIST\":\n",
    "    learning_rate = 3e-3\n",
    "elif dataset == 'DVS128':\n",
    "    learning_rate = 5e-3\n",
    "elif dataset == 'cifar10':\n",
    "    learning_rate = 1e-4\n",
    "else:\n",
    "    learning_rate = 1e-3\n",
    "precision_epochs = 150\n",
    "n_bits = 8\n",
    "warmup_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = folder +\"dataset_\" + str(data_index+1) + \"_timestep_\"+str(time_steps)+\"_head_\"+str(num_heads)+\"mlp_ratio\"+str(mlp_ratio)+f\"spsmodule_{spsmodule}\"+\"_encoder_\"+str(layers)+\"_embed_dims_\"+str(embed_dims)+\"_nbits_\"+str(n_bits)+\"_ep_\"+str(num_epoch)+\"_lr(e-3)_\"+str(learning_rate * 1000)+\"_seed_\"+str(seed)+\".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./result/DVS128/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = folder +f\"encoder_type_{encodermodule}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooldown = 10\n",
    "if dataset == \"DVS128\" :\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.epochs = num_epoch - cooldown\n",
    "            self.sched = 'step'  \n",
    "            self.min_lr = 1e-5\n",
    "            self.warmup_lr = 3e-4\n",
    "            self.warmup_epochs = 20\n",
    "            self.decay_rate = 0.9\n",
    "            self.cooldown_epochs = cooldown\n",
    "            self.lr_noise = [0.6, 0.9]\n",
    "            self.lr_noise_pct = 0.67\n",
    "            self.lr_noise_std = 1.0\n",
    "            self.seed = 42\n",
    "            self.decay_epochs = 20\n",
    "            self.patience_epochs = 5\n",
    "elif dataset == \"cifar10\":\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.epochs = num_epoch - cooldown\n",
    "            self.sched = 'step'  \n",
    "            self.min_lr = 1e-5\n",
    "            self.warmup_lr = 3e-5\n",
    "            self.warmup_epochs = 5\n",
    "            self.decay_rate = 0.9\n",
    "            self.cooldown_epochs = cooldown\n",
    "            self.lr_noise = [0.6, 0.9]\n",
    "            self.lr_noise_pct = 0.67\n",
    "            self.lr_noise_std = 1.0\n",
    "            self.seed = 42\n",
    "            self.decay_epochs = 20\n",
    "            self.patience_epochs = 5 \n",
    "else:\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.epochs = num_epoch - cooldown\n",
    "            self.sched = 'cosine'  \n",
    "            self.min_lr = 5e-5\n",
    "            self.warmup_lr = 3e-4\n",
    "            self.warmup_epochs = 20\n",
    "            self.decay_rate = 0.99\n",
    "            self.cooldown_epochs = cooldown\n",
    "            self.lr_noise = [0.6, 0.9]\n",
    "            self.lr_noise_pct = 0.67\n",
    "            self.lr_noise_std = 1.0\n",
    "            self.seed = 42\n",
    "            self.decay_epochs = 20\n",
    "            self.patience_epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam([{\"params\":base_params}, ], lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\":base_params}, ], lr=learning_rate, weight_decay=0)\n",
    "# optimizer = Lamb(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "if dataset == \"genomic\":\n",
    "#     scheduler = StepLR(optimizer, step_size=50, gamma=.6)\n",
    "    scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=5, verbose=True)\n",
    "elif dataset == \"MNIST\" or dataset == \"NMNIST\":\n",
    "#     scheduler = StepLR(optimizer, step_size=15, gamma=.8)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=3, verbose=True)\n",
    "    scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "elif dataset == \"DVS128\" or dataset == \"cifar10\":\n",
    "#     scheduler = StepLR(optimizer, step_size=5, gamma=.9)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, verbose=True)\n",
    "    scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "else:\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5, verbose=True)\n",
    "#     scheduler = StepLR(optimizer, step_size=10, gamma=.9)\n",
    "    scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "\n",
    "#     lambda1 = lambda epoch: 5 * epoch / 10 if epoch < 10 else 0.95 ** (epoch - 10)\n",
    "#     scheduler = LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "#     scheduler = create_scheduler(optimizer, 'cosine', num_epoch=num_epoch, decay_epochs=90, \n",
    "#                               cooldown_epochs=cooldown_epochs, decay_rate=0.1, min_lr=1e-10, warmup_lr=1e-6, \n",
    "#                               warmup_epochs=10, plateau_mode='max')\n",
    "# scheduler = lr_lambda(optimizer, T_max=num_epoch/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb optimization sweep hyperparameters\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfederico-corradi\u001b[0m (\u001b[33mnecs_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/federico/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='36511cd28a79d90c5bc87bc5745b7bff232c3eeb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization to efficiently explore the hyperparameter space\n",
    "    'metric': {\n",
    "        'name': 'test_accuracy',  # Primary metric to optimize\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 1e-4,\n",
    "            'max': 1e-3  # Learning rate range\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [12,16]  # Possible batch sizes\n",
    "        },\n",
    "        'time_steps': {\n",
    "            'values': [16]  # Add more values if needed\n",
    "        },\n",
    "        'embed_dims': {\n",
    "            'values': [256]  # Common options for embedding dimensions\n",
    "        },\n",
    "        'patch_size': {\n",
    "            'values': [8]  # Example values for patch sizes\n",
    "        },\n",
    "        'input_extend': {\n",
    "            'values': [1,4,8]  # Binary choice\n",
    "        },\n",
    "        'num_epoch': {\n",
    "            'values': [300]  # Experiment with different training durations\n",
    "        },\n",
    "        'thresholds': {\n",
    "            'values': [\n",
    "                [128/128, 128/128, 128/128],  # Current configuration\n",
    "                [64/128, 64/128, 64/128]    # Example alternative\n",
    "                #[192/128, 192/128, 192/128]  # Another example\n",
    "            ]\n",
    "        },\n",
    "        'pooling_stat': {\n",
    "            'values': [\"1111\"]  # Example configurations\n",
    "        },\n",
    "        'manual_seed': {\n",
    "            'min': 1,\n",
    "            'max': 250,  # Define the range of seeds to explore\n",
    "            'distribution': 'int_uniform'  # Ensure seeds are integers\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7zqyyfqk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-rain-1</strong> at: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/7zqyyfqk' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/7zqyyfqk</a><br/> View project at: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_125900-7zqyyfqk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7zqyyfqk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/federico/tue/jupyter/snntransformer/wandb/run-20241202_125929-48sparki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/48sparki' target=\"_blank\">splendid-shape-2</a></strong> to <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/48sparki' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/48sparki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/48sparki?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x724b320fc0a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize wandb run\n",
    "wandb.init(project=\"SpikeVisionSeedDBv1\", config={\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 50,\n",
    "    \"embed_dim\": 256,\n",
    "    \"alpha\": 0.9,\n",
    "    \"v_threshold\": 0.1,\n",
    "    \"scheduler_type\": \"cosine\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    max_timesteps = time_steps\n",
    "    padded_batch = []\n",
    "\n",
    "    for item in batch:\n",
    "        data, label = item\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data).float()\n",
    "\n",
    "        current_timesteps = data.size(0)\n",
    "\n",
    "        if current_timesteps < max_timesteps:\n",
    "            padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "            padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "        else:\n",
    "            padded_data = data[:max_timesteps]\n",
    "\n",
    "        padded_batch.append((padded_data, label))\n",
    "    return torch.utils.data.dataloader.default_collate(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: amzq1zqt\n",
      "Sweep URL: https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3hr21ex0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dims: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_extend: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009544711456481556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmanual_seed: 196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epoch: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling_stat: 1111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tthresholds: [0.5, 0.5, 0.5]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_steps: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msplendid-shape-2\u001b[0m at: \u001b[34mhttps://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/48sparki\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241202_125929-48sparki/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/federico/tue/jupyter/snntransformer/wandb/run-20241202_125935-3hr21ex0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3hr21ex0' target=\"_blank\">golden-sweep-1</a></strong> to <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3hr21ex0' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3hr21ex0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n",
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n",
      "Random Seed: 67\n",
      "Train Loader Length: 74\n",
      "Test Loader Length: 19\n",
      "Epoch [1/250], Train Loss: 2.3979, Train Accuracy: 8.71%, Test Loss: 0.1456, Test Accuracy: 15.70%\n",
      "Epoch [2/250], Train Loss: 1.9918, Train Accuracy: 33.65%, Test Loss: 0.1329, Test Accuracy: 55.63%\n",
      "Epoch [3/250], Train Loss: 1.6200, Train Accuracy: 48.08%, Test Loss: 0.1703, Test Accuracy: 46.08%\n",
      "Epoch [4/250], Train Loss: 1.4090, Train Accuracy: 55.42%, Test Loss: 0.1328, Test Accuracy: 58.02%\n",
      "Epoch [5/250], Train Loss: 1.4216, Train Accuracy: 55.59%, Test Loss: 0.1117, Test Accuracy: 61.77%\n",
      "Epoch [6/250], Train Loss: 1.2783, Train Accuracy: 56.02%, Test Loss: 0.1255, Test Accuracy: 57.68%\n",
      "Epoch [7/250], Train Loss: 1.2226, Train Accuracy: 59.44%, Test Loss: 0.1179, Test Accuracy: 59.39%\n",
      "Epoch [8/250], Train Loss: 1.1996, Train Accuracy: 59.44%, Test Loss: 0.1053, Test Accuracy: 61.09%\n",
      "Epoch [9/250], Train Loss: 1.1605, Train Accuracy: 62.34%, Test Loss: 0.0988, Test Accuracy: 60.07%\n",
      "Epoch [10/250], Train Loss: 1.1300, Train Accuracy: 62.68%, Test Loss: 0.0889, Test Accuracy: 66.89%\n",
      "Epoch [11/250], Train Loss: 1.0079, Train Accuracy: 66.61%, Test Loss: 0.0682, Test Accuracy: 71.67%\n",
      "Epoch [12/250], Train Loss: 1.0242, Train Accuracy: 66.01%, Test Loss: 0.0910, Test Accuracy: 67.24%\n",
      "Epoch [13/250], Train Loss: 0.9584, Train Accuracy: 68.06%, Test Loss: 0.0624, Test Accuracy: 75.09%\n",
      "Epoch [14/250], Train Loss: 0.9205, Train Accuracy: 70.79%, Test Loss: 0.0503, Test Accuracy: 81.57%\n",
      "Epoch [15/250], Train Loss: 0.8164, Train Accuracy: 74.64%, Test Loss: 0.0615, Test Accuracy: 76.45%\n",
      "Epoch [16/250], Train Loss: 0.7720, Train Accuracy: 74.89%, Test Loss: 0.0545, Test Accuracy: 79.18%\n",
      "Epoch [17/250], Train Loss: 0.7573, Train Accuracy: 76.35%, Test Loss: 0.0575, Test Accuracy: 77.13%\n",
      "Epoch [18/250], Train Loss: 0.7312, Train Accuracy: 77.88%, Test Loss: 0.0414, Test Accuracy: 80.89%\n",
      "Epoch [19/250], Train Loss: 0.6347, Train Accuracy: 80.70%, Test Loss: 0.0398, Test Accuracy: 82.94%\n",
      "Epoch [20/250], Train Loss: 0.6229, Train Accuracy: 80.70%, Test Loss: 0.0383, Test Accuracy: 84.30%\n",
      "Epoch [21/250], Train Loss: 0.6007, Train Accuracy: 80.79%, Test Loss: 0.0500, Test Accuracy: 80.89%\n",
      "Epoch [22/250], Train Loss: 0.6008, Train Accuracy: 81.90%, Test Loss: 0.0353, Test Accuracy: 86.69%\n",
      "Epoch [23/250], Train Loss: 0.5793, Train Accuracy: 82.49%, Test Loss: 0.0399, Test Accuracy: 84.64%\n",
      "Epoch [24/250], Train Loss: 0.5006, Train Accuracy: 84.54%, Test Loss: 0.0365, Test Accuracy: 84.64%\n",
      "Epoch [25/250], Train Loss: 0.5253, Train Accuracy: 83.77%, Test Loss: 0.0339, Test Accuracy: 84.30%\n",
      "Epoch [26/250], Train Loss: 0.4899, Train Accuracy: 85.40%, Test Loss: 0.0414, Test Accuracy: 85.67%\n",
      "Epoch [27/250], Train Loss: 0.4747, Train Accuracy: 85.74%, Test Loss: 0.0361, Test Accuracy: 87.37%\n",
      "Epoch [28/250], Train Loss: 0.4862, Train Accuracy: 85.74%, Test Loss: 0.0565, Test Accuracy: 82.25%\n",
      "Epoch [29/250], Train Loss: 0.4666, Train Accuracy: 86.25%, Test Loss: 0.0489, Test Accuracy: 82.94%\n",
      "Epoch [30/250], Train Loss: 0.5490, Train Accuracy: 83.69%, Test Loss: 0.0343, Test Accuracy: 86.35%\n",
      "Epoch [31/250], Train Loss: 0.4436, Train Accuracy: 87.45%, Test Loss: 0.0381, Test Accuracy: 86.01%\n",
      "Epoch [32/250], Train Loss: 0.4276, Train Accuracy: 86.93%, Test Loss: 0.0370, Test Accuracy: 84.98%\n",
      "Epoch [33/250], Train Loss: 0.4365, Train Accuracy: 87.02%, Test Loss: 0.0269, Test Accuracy: 88.74%\n",
      "New best model saved with test accuracy: 88.74%\n",
      "Epoch [34/250], Train Loss: 0.3804, Train Accuracy: 88.39%, Test Loss: 0.0284, Test Accuracy: 88.05%\n",
      "Epoch [35/250], Train Loss: 0.3770, Train Accuracy: 88.98%, Test Loss: 0.0326, Test Accuracy: 87.03%\n",
      "Epoch [36/250], Train Loss: 0.3498, Train Accuracy: 89.50%, Test Loss: 0.0278, Test Accuracy: 89.42%\n",
      "New best model saved with test accuracy: 89.42%\n",
      "Epoch [37/250], Train Loss: 0.3352, Train Accuracy: 90.09%, Test Loss: 0.0375, Test Accuracy: 85.67%\n",
      "Epoch [38/250], Train Loss: 0.3207, Train Accuracy: 90.78%, Test Loss: 0.0509, Test Accuracy: 81.57%\n",
      "Epoch [39/250], Train Loss: 0.3118, Train Accuracy: 91.55%, Test Loss: 0.0265, Test Accuracy: 91.13%\n",
      "New best model saved with test accuracy: 91.13%\n",
      "Epoch [40/250], Train Loss: 0.3490, Train Accuracy: 90.95%, Test Loss: 0.0261, Test Accuracy: 90.10%\n",
      "Epoch [41/250], Train Loss: 0.2979, Train Accuracy: 92.31%, Test Loss: 0.0275, Test Accuracy: 89.08%\n",
      "Epoch [42/250], Train Loss: 0.3447, Train Accuracy: 90.44%, Test Loss: 0.0263, Test Accuracy: 90.78%\n",
      "Epoch [43/250], Train Loss: 0.3059, Train Accuracy: 91.63%, Test Loss: 0.0258, Test Accuracy: 90.44%\n",
      "Epoch [44/250], Train Loss: 0.3049, Train Accuracy: 91.37%, Test Loss: 0.0314, Test Accuracy: 89.42%\n",
      "Epoch [45/250], Train Loss: 0.2782, Train Accuracy: 92.83%, Test Loss: 0.0216, Test Accuracy: 92.49%\n",
      "New best model saved with test accuracy: 92.49%\n",
      "Epoch [46/250], Train Loss: 0.2299, Train Accuracy: 94.11%, Test Loss: 0.0231, Test Accuracy: 90.78%\n",
      "Epoch [47/250], Train Loss: 0.2719, Train Accuracy: 92.83%, Test Loss: 0.0285, Test Accuracy: 89.76%\n",
      "Epoch [48/250], Train Loss: 0.2716, Train Accuracy: 92.91%, Test Loss: 0.0237, Test Accuracy: 90.44%\n",
      "Epoch [49/250], Train Loss: 0.2845, Train Accuracy: 92.31%, Test Loss: 0.0327, Test Accuracy: 87.71%\n",
      "Epoch [50/250], Train Loss: 0.2529, Train Accuracy: 93.34%, Test Loss: 0.0284, Test Accuracy: 89.42%\n",
      "Epoch [51/250], Train Loss: 0.2565, Train Accuracy: 93.17%, Test Loss: 0.0206, Test Accuracy: 92.15%\n",
      "Epoch [52/250], Train Loss: 0.2316, Train Accuracy: 94.11%, Test Loss: 0.0218, Test Accuracy: 93.52%\n",
      "New best model saved with test accuracy: 93.52%\n",
      "Epoch [53/250], Train Loss: 0.2476, Train Accuracy: 93.60%, Test Loss: 0.0220, Test Accuracy: 91.81%\n",
      "Epoch [54/250], Train Loss: 0.2176, Train Accuracy: 94.45%, Test Loss: 0.0225, Test Accuracy: 91.13%\n",
      "Epoch [55/250], Train Loss: 0.2937, Train Accuracy: 92.23%, Test Loss: 0.0278, Test Accuracy: 88.74%\n",
      "Epoch [56/250], Train Loss: 0.2215, Train Accuracy: 95.05%, Test Loss: 0.0228, Test Accuracy: 91.81%\n",
      "Epoch [57/250], Train Loss: 0.2032, Train Accuracy: 95.22%, Test Loss: 0.0206, Test Accuracy: 91.81%\n",
      "Epoch [58/250], Train Loss: 0.2419, Train Accuracy: 93.85%, Test Loss: 0.0173, Test Accuracy: 93.52%\n",
      "Epoch [59/250], Train Loss: 0.2197, Train Accuracy: 94.88%, Test Loss: 0.0357, Test Accuracy: 88.05%\n",
      "Epoch [60/250], Train Loss: 0.2298, Train Accuracy: 94.53%, Test Loss: 0.0244, Test Accuracy: 90.10%\n",
      "Epoch [61/250], Train Loss: 0.2034, Train Accuracy: 94.96%, Test Loss: 0.0212, Test Accuracy: 93.17%\n",
      "Epoch [62/250], Train Loss: 0.2107, Train Accuracy: 94.79%, Test Loss: 0.0235, Test Accuracy: 90.44%\n",
      "Epoch [63/250], Train Loss: 0.1740, Train Accuracy: 96.24%, Test Loss: 0.0184, Test Accuracy: 92.83%\n",
      "Epoch [64/250], Train Loss: 0.2028, Train Accuracy: 94.71%, Test Loss: 0.0203, Test Accuracy: 91.13%\n",
      "Epoch [65/250], Train Loss: 0.1870, Train Accuracy: 96.07%, Test Loss: 0.0223, Test Accuracy: 92.49%\n",
      "Epoch [66/250], Train Loss: 0.1664, Train Accuracy: 96.93%, Test Loss: 0.0320, Test Accuracy: 88.05%\n",
      "Epoch [67/250], Train Loss: 0.2151, Train Accuracy: 94.96%, Test Loss: 0.0251, Test Accuracy: 90.44%\n",
      "Epoch [68/250], Train Loss: 0.1691, Train Accuracy: 96.50%, Test Loss: 0.0284, Test Accuracy: 89.42%\n",
      "Epoch [69/250], Train Loss: 0.2181, Train Accuracy: 94.62%, Test Loss: 0.0202, Test Accuracy: 91.81%\n",
      "Epoch [70/250], Train Loss: 0.2182, Train Accuracy: 94.62%, Test Loss: 0.0164, Test Accuracy: 93.17%\n",
      "Epoch [71/250], Train Loss: 0.2000, Train Accuracy: 94.88%, Test Loss: 0.0189, Test Accuracy: 91.81%\n",
      "Epoch [72/250], Train Loss: 0.1936, Train Accuracy: 95.73%, Test Loss: 0.0229, Test Accuracy: 90.44%\n",
      "Epoch [73/250], Train Loss: 0.2063, Train Accuracy: 95.73%, Test Loss: 0.0199, Test Accuracy: 91.81%\n",
      "Epoch [74/250], Train Loss: 0.1668, Train Accuracy: 97.35%, Test Loss: 0.0211, Test Accuracy: 92.83%\n",
      "Epoch [75/250], Train Loss: 0.1515, Train Accuracy: 96.50%, Test Loss: 0.0197, Test Accuracy: 92.15%\n",
      "Epoch [76/250], Train Loss: 0.1408, Train Accuracy: 97.69%, Test Loss: 0.0181, Test Accuracy: 93.52%\n",
      "Epoch [77/250], Train Loss: 0.1586, Train Accuracy: 97.01%, Test Loss: 0.0184, Test Accuracy: 91.81%\n",
      "Epoch [78/250], Train Loss: 0.1342, Train Accuracy: 97.95%, Test Loss: 0.0217, Test Accuracy: 92.15%\n",
      "Epoch [79/250], Train Loss: 0.1434, Train Accuracy: 97.87%, Test Loss: 0.0205, Test Accuracy: 92.49%\n",
      "Epoch [80/250], Train Loss: 0.1468, Train Accuracy: 97.44%, Test Loss: 0.0237, Test Accuracy: 90.78%\n",
      "Epoch [81/250], Train Loss: 0.1378, Train Accuracy: 97.01%, Test Loss: 0.0223, Test Accuracy: 92.49%\n",
      "Epoch [82/250], Train Loss: 0.1531, Train Accuracy: 96.75%, Test Loss: 0.0249, Test Accuracy: 91.13%\n",
      "Epoch [83/250], Train Loss: 0.1959, Train Accuracy: 94.96%, Test Loss: 0.0208, Test Accuracy: 89.76%\n",
      "Epoch [84/250], Train Loss: 0.1713, Train Accuracy: 96.33%, Test Loss: 0.0186, Test Accuracy: 91.13%\n",
      "Epoch [85/250], Train Loss: 0.1620, Train Accuracy: 96.24%, Test Loss: 0.0272, Test Accuracy: 87.71%\n",
      "Epoch [86/250], Train Loss: 0.2149, Train Accuracy: 94.62%, Test Loss: 0.0225, Test Accuracy: 92.83%\n",
      "Epoch [87/250], Train Loss: 0.1693, Train Accuracy: 97.01%, Test Loss: 0.0250, Test Accuracy: 90.78%\n",
      "Epoch [88/250], Train Loss: 0.1641, Train Accuracy: 96.33%, Test Loss: 0.0144, Test Accuracy: 92.15%\n",
      "Epoch [89/250], Train Loss: 0.1434, Train Accuracy: 97.18%, Test Loss: 0.0210, Test Accuracy: 90.78%\n",
      "Epoch [90/250], Train Loss: 0.1454, Train Accuracy: 97.44%, Test Loss: 0.0186, Test Accuracy: 92.49%\n",
      "Epoch [91/250], Train Loss: 0.1512, Train Accuracy: 96.58%, Test Loss: 0.0239, Test Accuracy: 91.47%\n",
      "Epoch [92/250], Train Loss: 0.1311, Train Accuracy: 97.44%, Test Loss: 0.0192, Test Accuracy: 91.13%\n",
      "Epoch [93/250], Train Loss: 0.1551, Train Accuracy: 96.41%, Test Loss: 0.0227, Test Accuracy: 93.52%\n",
      "Epoch [94/250], Train Loss: 0.1395, Train Accuracy: 97.52%, Test Loss: 0.0150, Test Accuracy: 93.86%\n",
      "New best model saved with test accuracy: 93.86%\n",
      "Epoch [95/250], Train Loss: 0.1595, Train Accuracy: 96.84%, Test Loss: 0.0130, Test Accuracy: 93.86%\n",
      "Epoch [96/250], Train Loss: 0.1374, Train Accuracy: 97.35%, Test Loss: 0.0122, Test Accuracy: 95.22%\n",
      "New best model saved with test accuracy: 95.22%\n",
      "Epoch [97/250], Train Loss: 0.1437, Train Accuracy: 96.93%, Test Loss: 0.0152, Test Accuracy: 92.49%\n",
      "Epoch [98/250], Train Loss: 0.1296, Train Accuracy: 97.87%, Test Loss: 0.0120, Test Accuracy: 94.54%\n",
      "Epoch [99/250], Train Loss: 0.1198, Train Accuracy: 97.78%, Test Loss: 0.0171, Test Accuracy: 94.20%\n",
      "Epoch [100/250], Train Loss: 0.1193, Train Accuracy: 98.21%, Test Loss: 0.0199, Test Accuracy: 93.86%\n",
      "Epoch [101/250], Train Loss: 0.1133, Train Accuracy: 98.46%, Test Loss: 0.0190, Test Accuracy: 93.52%\n",
      "Epoch [102/250], Train Loss: 0.1662, Train Accuracy: 96.16%, Test Loss: 0.0202, Test Accuracy: 92.83%\n",
      "Epoch [103/250], Train Loss: 0.1438, Train Accuracy: 97.18%, Test Loss: 0.0205, Test Accuracy: 92.49%\n",
      "Epoch [104/250], Train Loss: 0.1779, Train Accuracy: 95.82%, Test Loss: 0.0213, Test Accuracy: 92.49%\n",
      "Epoch [105/250], Train Loss: 0.1437, Train Accuracy: 97.10%, Test Loss: 0.0160, Test Accuracy: 94.54%\n",
      "Epoch [106/250], Train Loss: 0.1277, Train Accuracy: 97.69%, Test Loss: 0.0181, Test Accuracy: 93.52%\n",
      "Epoch [107/250], Train Loss: 0.1229, Train Accuracy: 98.12%, Test Loss: 0.0215, Test Accuracy: 91.81%\n",
      "Epoch [108/250], Train Loss: 0.1310, Train Accuracy: 97.52%, Test Loss: 0.0184, Test Accuracy: 93.17%\n",
      "Epoch [109/250], Train Loss: 0.1338, Train Accuracy: 97.52%, Test Loss: 0.0129, Test Accuracy: 94.54%\n",
      "Epoch [110/250], Train Loss: 0.1211, Train Accuracy: 98.21%, Test Loss: 0.0207, Test Accuracy: 92.15%\n",
      "Epoch [111/250], Train Loss: 0.1391, Train Accuracy: 97.52%, Test Loss: 0.0221, Test Accuracy: 90.44%\n",
      "Epoch [112/250], Train Loss: 0.1054, Train Accuracy: 98.63%, Test Loss: 0.0152, Test Accuracy: 93.86%\n",
      "Epoch [113/250], Train Loss: 0.1190, Train Accuracy: 98.46%, Test Loss: 0.0200, Test Accuracy: 92.49%\n",
      "Epoch [114/250], Train Loss: 0.1159, Train Accuracy: 98.55%, Test Loss: 0.0144, Test Accuracy: 93.17%\n",
      "Epoch [115/250], Train Loss: 0.1120, Train Accuracy: 98.72%, Test Loss: 0.0113, Test Accuracy: 94.54%\n",
      "Epoch [116/250], Train Loss: 0.0980, Train Accuracy: 98.29%, Test Loss: 0.0164, Test Accuracy: 92.49%\n",
      "Epoch [117/250], Train Loss: 0.1393, Train Accuracy: 97.35%, Test Loss: 0.0453, Test Accuracy: 83.62%\n",
      "Epoch [118/250], Train Loss: 0.1401, Train Accuracy: 97.44%, Test Loss: 0.0141, Test Accuracy: 93.52%\n",
      "Epoch [119/250], Train Loss: 0.1076, Train Accuracy: 98.89%, Test Loss: 0.0178, Test Accuracy: 92.15%\n",
      "Epoch [120/250], Train Loss: 0.1053, Train Accuracy: 98.89%, Test Loss: 0.0186, Test Accuracy: 92.15%\n",
      "Epoch [121/250], Train Loss: 0.1087, Train Accuracy: 98.29%, Test Loss: 0.0144, Test Accuracy: 93.86%\n",
      "Epoch [122/250], Train Loss: 0.0996, Train Accuracy: 98.80%, Test Loss: 0.0141, Test Accuracy: 94.54%\n",
      "Epoch [123/250], Train Loss: 0.1252, Train Accuracy: 98.38%, Test Loss: 0.0145, Test Accuracy: 92.83%\n",
      "Epoch [124/250], Train Loss: 0.1004, Train Accuracy: 98.80%, Test Loss: 0.0205, Test Accuracy: 93.86%\n",
      "Epoch [125/250], Train Loss: 0.0940, Train Accuracy: 98.89%, Test Loss: 0.0187, Test Accuracy: 91.81%\n",
      "Epoch [126/250], Train Loss: 0.0984, Train Accuracy: 98.46%, Test Loss: 0.0109, Test Accuracy: 94.88%\n",
      "Epoch [127/250], Train Loss: 0.1141, Train Accuracy: 98.38%, Test Loss: 0.0134, Test Accuracy: 92.49%\n",
      "Epoch [128/250], Train Loss: 0.1024, Train Accuracy: 98.98%, Test Loss: 0.0134, Test Accuracy: 94.20%\n",
      "Epoch [129/250], Train Loss: 0.1065, Train Accuracy: 98.72%, Test Loss: 0.0152, Test Accuracy: 94.88%\n",
      "Epoch [130/250], Train Loss: 0.0979, Train Accuracy: 98.98%, Test Loss: 0.0190, Test Accuracy: 92.83%\n",
      "Epoch [131/250], Train Loss: 0.1253, Train Accuracy: 97.87%, Test Loss: 0.0221, Test Accuracy: 92.83%\n",
      "Epoch [132/250], Train Loss: 0.1357, Train Accuracy: 98.04%, Test Loss: 0.0230, Test Accuracy: 90.78%\n",
      "Epoch [133/250], Train Loss: 0.0960, Train Accuracy: 98.89%, Test Loss: 0.0162, Test Accuracy: 93.86%\n",
      "Epoch [134/250], Train Loss: 0.1061, Train Accuracy: 98.80%, Test Loss: 0.0131, Test Accuracy: 93.52%\n",
      "Epoch [135/250], Train Loss: 0.1080, Train Accuracy: 98.46%, Test Loss: 0.0177, Test Accuracy: 92.49%\n",
      "Epoch [136/250], Train Loss: 0.1051, Train Accuracy: 98.46%, Test Loss: 0.0228, Test Accuracy: 92.49%\n",
      "Epoch [137/250], Train Loss: 0.0968, Train Accuracy: 98.72%, Test Loss: 0.0214, Test Accuracy: 91.47%\n",
      "Epoch [138/250], Train Loss: 0.0848, Train Accuracy: 99.06%, Test Loss: 0.0153, Test Accuracy: 95.22%\n",
      "Epoch [139/250], Train Loss: 0.0972, Train Accuracy: 98.46%, Test Loss: 0.0170, Test Accuracy: 93.86%\n",
      "Epoch [140/250], Train Loss: 0.0988, Train Accuracy: 98.89%, Test Loss: 0.0126, Test Accuracy: 94.88%\n",
      "Epoch [141/250], Train Loss: 0.0929, Train Accuracy: 98.98%, Test Loss: 0.0139, Test Accuracy: 94.20%\n",
      "Epoch [142/250], Train Loss: 0.0876, Train Accuracy: 99.23%, Test Loss: 0.0448, Test Accuracy: 85.32%\n",
      "Epoch [143/250], Train Loss: 0.0958, Train Accuracy: 98.72%, Test Loss: 0.0156, Test Accuracy: 93.86%\n",
      "Epoch [144/250], Train Loss: 0.1025, Train Accuracy: 98.29%, Test Loss: 0.0154, Test Accuracy: 93.17%\n",
      "Epoch [145/250], Train Loss: 0.1132, Train Accuracy: 98.38%, Test Loss: 0.0125, Test Accuracy: 94.88%\n",
      "Epoch [146/250], Train Loss: 0.1047, Train Accuracy: 98.46%, Test Loss: 0.0187, Test Accuracy: 92.15%\n",
      "Epoch [147/250], Train Loss: 0.0879, Train Accuracy: 98.98%, Test Loss: 0.0148, Test Accuracy: 94.88%\n",
      "Epoch [148/250], Train Loss: 0.0940, Train Accuracy: 98.89%, Test Loss: 0.0226, Test Accuracy: 92.83%\n",
      "Epoch [149/250], Train Loss: 0.0838, Train Accuracy: 98.98%, Test Loss: 0.0097, Test Accuracy: 96.25%\n",
      "New best model saved with test accuracy: 96.25%\n",
      "Epoch [150/250], Train Loss: 0.0861, Train Accuracy: 98.98%, Test Loss: 0.0115, Test Accuracy: 95.56%\n",
      "Epoch [151/250], Train Loss: 0.1269, Train Accuracy: 97.52%, Test Loss: 0.0229, Test Accuracy: 89.76%\n",
      "Epoch [152/250], Train Loss: 0.1641, Train Accuracy: 96.33%, Test Loss: 0.0141, Test Accuracy: 94.54%\n",
      "Epoch [153/250], Train Loss: 0.0993, Train Accuracy: 98.63%, Test Loss: 0.0121, Test Accuracy: 95.22%\n",
      "Epoch [154/250], Train Loss: 0.0996, Train Accuracy: 98.21%, Test Loss: 0.0174, Test Accuracy: 93.17%\n",
      "Epoch [155/250], Train Loss: 0.0984, Train Accuracy: 98.46%, Test Loss: 0.0205, Test Accuracy: 90.78%\n",
      "Epoch [156/250], Train Loss: 0.0812, Train Accuracy: 99.23%, Test Loss: 0.0232, Test Accuracy: 93.17%\n",
      "Epoch [157/250], Train Loss: 0.0811, Train Accuracy: 99.06%, Test Loss: 0.0146, Test Accuracy: 92.49%\n",
      "Epoch [158/250], Train Loss: 0.0906, Train Accuracy: 98.72%, Test Loss: 0.0179, Test Accuracy: 93.86%\n",
      "Epoch [159/250], Train Loss: 0.1076, Train Accuracy: 98.46%, Test Loss: 0.0183, Test Accuracy: 91.47%\n",
      "Epoch [160/250], Train Loss: 0.1036, Train Accuracy: 98.55%, Test Loss: 0.0209, Test Accuracy: 91.81%\n",
      "Epoch [161/250], Train Loss: 0.1094, Train Accuracy: 98.38%, Test Loss: 0.0189, Test Accuracy: 94.20%\n",
      "Epoch [162/250], Train Loss: 0.0949, Train Accuracy: 98.63%, Test Loss: 0.0150, Test Accuracy: 93.86%\n",
      "Epoch [163/250], Train Loss: 0.0798, Train Accuracy: 99.49%, Test Loss: 0.0153, Test Accuracy: 93.52%\n",
      "Epoch [164/250], Train Loss: 0.0815, Train Accuracy: 99.49%, Test Loss: 0.0190, Test Accuracy: 91.13%\n",
      "Epoch [165/250], Train Loss: 0.0769, Train Accuracy: 99.49%, Test Loss: 0.0205, Test Accuracy: 93.17%\n",
      "Epoch [166/250], Train Loss: 0.0872, Train Accuracy: 98.89%, Test Loss: 0.0219, Test Accuracy: 91.13%\n",
      "Epoch [167/250], Train Loss: 0.0934, Train Accuracy: 98.72%, Test Loss: 0.0220, Test Accuracy: 88.74%\n",
      "Epoch [168/250], Train Loss: 0.0980, Train Accuracy: 98.38%, Test Loss: 0.0181, Test Accuracy: 91.81%\n",
      "Epoch [169/250], Train Loss: 0.1171, Train Accuracy: 98.29%, Test Loss: 0.0194, Test Accuracy: 91.81%\n",
      "Epoch [170/250], Train Loss: 0.0843, Train Accuracy: 98.98%, Test Loss: 0.0137, Test Accuracy: 94.88%\n",
      "Epoch [171/250], Train Loss: 0.2230, Train Accuracy: 94.88%, Test Loss: 0.0258, Test Accuracy: 90.78%\n",
      "Epoch [172/250], Train Loss: 0.0788, Train Accuracy: 99.49%, Test Loss: 0.0134, Test Accuracy: 94.20%\n",
      "Epoch [173/250], Train Loss: 0.0804, Train Accuracy: 99.23%, Test Loss: 0.0167, Test Accuracy: 92.15%\n",
      "Epoch [174/250], Train Loss: 0.0770, Train Accuracy: 99.32%, Test Loss: 0.0183, Test Accuracy: 92.83%\n",
      "Epoch [175/250], Train Loss: 0.0790, Train Accuracy: 99.15%, Test Loss: 0.0141, Test Accuracy: 94.54%\n",
      "Epoch [176/250], Train Loss: 0.0807, Train Accuracy: 99.15%, Test Loss: 0.0086, Test Accuracy: 97.27%\n",
      "New best model saved with test accuracy: 97.27%\n",
      "Epoch [177/250], Train Loss: 0.0747, Train Accuracy: 99.40%, Test Loss: 0.0159, Test Accuracy: 94.54%\n",
      "Epoch [178/250], Train Loss: 0.0757, Train Accuracy: 99.57%, Test Loss: 0.0163, Test Accuracy: 94.20%\n",
      "Epoch [179/250], Train Loss: 0.0754, Train Accuracy: 99.23%, Test Loss: 0.0144, Test Accuracy: 94.54%\n",
      "Epoch [180/250], Train Loss: 0.0795, Train Accuracy: 99.15%, Test Loss: 0.0124, Test Accuracy: 94.88%\n",
      "Epoch [181/250], Train Loss: 0.0789, Train Accuracy: 99.15%, Test Loss: 0.0173, Test Accuracy: 93.17%\n",
      "Epoch [182/250], Train Loss: 0.0674, Train Accuracy: 99.66%, Test Loss: 0.0111, Test Accuracy: 95.90%\n",
      "Epoch [183/250], Train Loss: 0.0711, Train Accuracy: 99.83%, Test Loss: 0.0158, Test Accuracy: 93.17%\n",
      "Epoch [184/250], Train Loss: 0.0657, Train Accuracy: 99.57%, Test Loss: 0.0149, Test Accuracy: 93.17%\n",
      "Epoch [185/250], Train Loss: 0.0697, Train Accuracy: 99.23%, Test Loss: 0.0151, Test Accuracy: 93.17%\n",
      "Epoch [186/250], Train Loss: 0.1187, Train Accuracy: 98.04%, Test Loss: 0.0228, Test Accuracy: 91.47%\n",
      "Epoch [187/250], Train Loss: 0.1057, Train Accuracy: 98.29%, Test Loss: 0.0139, Test Accuracy: 95.56%\n",
      "Epoch [188/250], Train Loss: 0.0901, Train Accuracy: 98.98%, Test Loss: 0.0132, Test Accuracy: 94.20%\n",
      "Epoch [189/250], Train Loss: 0.0799, Train Accuracy: 99.15%, Test Loss: 0.0125, Test Accuracy: 93.52%\n",
      "Epoch [190/250], Train Loss: 0.0675, Train Accuracy: 99.57%, Test Loss: 0.0121, Test Accuracy: 95.90%\n",
      "Epoch [191/250], Train Loss: 0.0651, Train Accuracy: 99.57%, Test Loss: 0.0170, Test Accuracy: 94.54%\n",
      "Epoch [192/250], Train Loss: 0.0660, Train Accuracy: 99.57%, Test Loss: 0.0100, Test Accuracy: 96.93%\n",
      "Epoch [193/250], Train Loss: 0.0683, Train Accuracy: 99.83%, Test Loss: 0.0104, Test Accuracy: 94.54%\n",
      "Epoch [194/250], Train Loss: 0.0772, Train Accuracy: 99.23%, Test Loss: 0.0159, Test Accuracy: 93.17%\n",
      "Epoch [195/250], Train Loss: 0.0819, Train Accuracy: 99.49%, Test Loss: 0.0078, Test Accuracy: 95.90%\n",
      "Epoch [196/250], Train Loss: 0.0824, Train Accuracy: 98.98%, Test Loss: 0.0149, Test Accuracy: 93.86%\n",
      "Epoch [197/250], Train Loss: 0.0921, Train Accuracy: 98.46%, Test Loss: 0.0134, Test Accuracy: 92.49%\n",
      "Epoch [198/250], Train Loss: 0.0685, Train Accuracy: 99.74%, Test Loss: 0.0126, Test Accuracy: 95.22%\n",
      "Epoch [199/250], Train Loss: 0.0801, Train Accuracy: 99.57%, Test Loss: 0.0113, Test Accuracy: 95.56%\n",
      "Epoch [200/250], Train Loss: 0.0632, Train Accuracy: 99.57%, Test Loss: 0.0147, Test Accuracy: 94.88%\n",
      "Epoch [201/250], Train Loss: 0.0634, Train Accuracy: 99.49%, Test Loss: 0.0096, Test Accuracy: 96.25%\n",
      "Epoch [202/250], Train Loss: 0.0661, Train Accuracy: 99.66%, Test Loss: 0.0094, Test Accuracy: 96.93%\n",
      "Epoch [203/250], Train Loss: 0.0612, Train Accuracy: 99.83%, Test Loss: 0.0077, Test Accuracy: 96.25%\n",
      "Epoch [204/250], Train Loss: 0.0784, Train Accuracy: 99.06%, Test Loss: 0.0148, Test Accuracy: 93.17%\n",
      "Epoch [205/250], Train Loss: 0.0706, Train Accuracy: 99.57%, Test Loss: 0.0114, Test Accuracy: 95.56%\n",
      "Epoch [206/250], Train Loss: 0.0675, Train Accuracy: 99.83%, Test Loss: 0.0085, Test Accuracy: 96.25%\n",
      "Epoch [207/250], Train Loss: 0.0676, Train Accuracy: 99.40%, Test Loss: 0.0256, Test Accuracy: 90.44%\n",
      "Epoch [208/250], Train Loss: 0.0860, Train Accuracy: 99.15%, Test Loss: 0.0101, Test Accuracy: 95.22%\n",
      "Epoch [209/250], Train Loss: 0.0783, Train Accuracy: 99.40%, Test Loss: 0.0113, Test Accuracy: 95.22%\n",
      "Epoch [210/250], Train Loss: 0.1831, Train Accuracy: 95.05%, Test Loss: 0.0359, Test Accuracy: 88.40%\n",
      "Epoch [211/250], Train Loss: 0.1190, Train Accuracy: 97.69%, Test Loss: 0.0194, Test Accuracy: 94.20%\n",
      "Epoch [212/250], Train Loss: 0.0927, Train Accuracy: 98.63%, Test Loss: 0.0315, Test Accuracy: 88.40%\n",
      "Epoch [213/250], Train Loss: 0.0748, Train Accuracy: 99.23%, Test Loss: 0.0105, Test Accuracy: 95.22%\n",
      "Epoch [214/250], Train Loss: 0.0591, Train Accuracy: 99.74%, Test Loss: 0.0129, Test Accuracy: 94.54%\n",
      "Epoch [215/250], Train Loss: 0.0647, Train Accuracy: 99.66%, Test Loss: 0.0137, Test Accuracy: 94.20%\n",
      "Epoch [216/250], Train Loss: 0.0690, Train Accuracy: 99.40%, Test Loss: 0.0089, Test Accuracy: 95.56%\n",
      "Epoch [217/250], Train Loss: 0.0672, Train Accuracy: 99.57%, Test Loss: 0.0093, Test Accuracy: 95.90%\n",
      "Epoch [218/250], Train Loss: 0.0665, Train Accuracy: 99.57%, Test Loss: 0.0157, Test Accuracy: 93.17%\n",
      "Epoch [219/250], Train Loss: 0.0641, Train Accuracy: 99.57%, Test Loss: 0.0127, Test Accuracy: 94.20%\n",
      "Epoch [220/250], Train Loss: 0.0672, Train Accuracy: 99.23%, Test Loss: 0.0119, Test Accuracy: 94.20%\n",
      "Epoch [221/250], Train Loss: 0.0609, Train Accuracy: 99.91%, Test Loss: 0.0183, Test Accuracy: 93.86%\n",
      "Epoch [222/250], Train Loss: 0.0614, Train Accuracy: 99.83%, Test Loss: 0.0071, Test Accuracy: 96.93%\n",
      "Epoch [223/250], Train Loss: 0.0553, Train Accuracy: 99.74%, Test Loss: 0.0114, Test Accuracy: 95.22%\n",
      "Epoch [224/250], Train Loss: 0.0611, Train Accuracy: 99.83%, Test Loss: 0.0109, Test Accuracy: 93.86%\n",
      "Epoch [225/250], Train Loss: 0.0593, Train Accuracy: 99.91%, Test Loss: 0.0145, Test Accuracy: 93.86%\n",
      "Epoch [226/250], Train Loss: 0.0633, Train Accuracy: 100.00%, Test Loss: 0.0108, Test Accuracy: 95.90%\n",
      "Epoch [227/250], Train Loss: 0.0550, Train Accuracy: 99.91%, Test Loss: 0.0205, Test Accuracy: 92.83%\n",
      "Epoch [228/250], Train Loss: 0.0739, Train Accuracy: 99.23%, Test Loss: 0.0150, Test Accuracy: 93.17%\n",
      "Epoch [229/250], Train Loss: 0.0646, Train Accuracy: 99.57%, Test Loss: 0.0179, Test Accuracy: 92.49%\n",
      "Epoch [230/250], Train Loss: 0.0601, Train Accuracy: 99.91%, Test Loss: 0.0103, Test Accuracy: 94.88%\n",
      "Epoch [231/250], Train Loss: 0.0584, Train Accuracy: 99.83%, Test Loss: 0.0130, Test Accuracy: 95.22%\n",
      "Epoch [232/250], Train Loss: 0.0706, Train Accuracy: 99.40%, Test Loss: 0.0180, Test Accuracy: 93.17%\n",
      "Epoch [233/250], Train Loss: 0.1831, Train Accuracy: 94.88%, Test Loss: 0.0199, Test Accuracy: 93.17%\n",
      "Epoch [234/250], Train Loss: 0.1067, Train Accuracy: 98.12%, Test Loss: 0.0206, Test Accuracy: 92.83%\n",
      "Epoch [235/250], Train Loss: 0.0789, Train Accuracy: 99.40%, Test Loss: 0.0098, Test Accuracy: 96.25%\n",
      "Epoch [236/250], Train Loss: 0.0942, Train Accuracy: 98.63%, Test Loss: 0.0121, Test Accuracy: 95.22%\n",
      "Epoch [237/250], Train Loss: 0.0782, Train Accuracy: 99.23%, Test Loss: 0.0100, Test Accuracy: 95.90%\n",
      "Epoch [238/250], Train Loss: 0.0738, Train Accuracy: 99.66%, Test Loss: 0.0107, Test Accuracy: 93.86%\n",
      "Epoch [239/250], Train Loss: 0.0742, Train Accuracy: 99.23%, Test Loss: 0.0178, Test Accuracy: 93.17%\n",
      "Epoch [240/250], Train Loss: 0.0538, Train Accuracy: 99.57%, Test Loss: 0.0144, Test Accuracy: 93.17%\n",
      "Epoch [241/250], Train Loss: 0.0585, Train Accuracy: 99.74%, Test Loss: 0.0106, Test Accuracy: 95.90%\n",
      "Epoch [242/250], Train Loss: 0.0602, Train Accuracy: 99.83%, Test Loss: 0.0118, Test Accuracy: 94.20%\n",
      "Epoch [243/250], Train Loss: 0.0635, Train Accuracy: 99.57%, Test Loss: 0.0095, Test Accuracy: 96.59%\n",
      "Epoch [244/250], Train Loss: 0.0689, Train Accuracy: 99.66%, Test Loss: 0.0113, Test Accuracy: 94.20%\n",
      "Epoch [245/250], Train Loss: 0.0660, Train Accuracy: 99.66%, Test Loss: 0.0096, Test Accuracy: 95.90%\n",
      "Epoch [246/250], Train Loss: 0.0617, Train Accuracy: 99.74%, Test Loss: 0.0166, Test Accuracy: 93.17%\n",
      "Epoch [247/250], Train Loss: 0.0672, Train Accuracy: 99.66%, Test Loss: 0.0099, Test Accuracy: 95.22%\n",
      "Epoch [248/250], Train Loss: 0.0629, Train Accuracy: 100.00%, Test Loss: 0.0111, Test Accuracy: 95.22%\n",
      "Epoch [249/250], Train Loss: 0.0631, Train Accuracy: 99.57%, Test Loss: 0.0211, Test Accuracy: 93.86%\n",
      "Epoch [250/250], Train Loss: 0.0571, Train Accuracy: 99.66%, Test Loss: 0.0127, Test Accuracy: 94.88%\n",
      "Training complete. Best test accuracy: 97.27%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>embed_dims</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇██████</td></tr><tr><td>input_extend</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>layers</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>manual_seed</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mlp_ratio</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>num_heads</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>patch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▃▃▅▆▇▆▇▇▇▇▇▇▇▇▇▇█▇▆▇▇███▇▇▇█▇██████▇███</td></tr><tr><td>test_loss</td><td>██▇▆▄▃▃▃▃▄▂▂▂▃▂▂▂▂▂▁▁▂▂▁▂▁▂▂▂▂▂▂▁▁▁▁▁▁▂▂</td></tr><tr><td>time_steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▅▅▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>16</td></tr><tr><td>embed_dims</td><td>256</td></tr><tr><td>epoch</td><td>250</td></tr><tr><td>input_extend</td><td>8</td></tr><tr><td>layers</td><td>2</td></tr><tr><td>learning_rate</td><td>0.00095</td></tr><tr><td>manual_seed</td><td>196</td></tr><tr><td>mlp_ratio</td><td>2</td></tr><tr><td>num_epoch</td><td>250</td></tr><tr><td>num_heads</td><td>1</td></tr><tr><td>patch_size</td><td>8</td></tr><tr><td>pooling_stat</td><td>1111</td></tr><tr><td>test_accuracy</td><td>94.88055</td></tr><tr><td>test_loss</td><td>0.01271</td></tr><tr><td>time_steps</td><td>16</td></tr><tr><td>train_accuracy</td><td>99.65841</td></tr><tr><td>train_loss</td><td>0.05706</td></tr><tr><td>train_threshold</td><td>False</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-1</strong> at: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3hr21ex0' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3hr21ex0</a><br/> View project at: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 9 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241202_125935-3hr21ex0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3osw89lt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dims: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_extend: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000976521069180304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmanual_seed: 197\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epoch: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpooling_stat: 1111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tthresholds: [1, 1, 1]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_steps: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/federico/tue/jupyter/snntransformer/wandb/run-20241202_153958-3osw89lt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3osw89lt' target=\"_blank\">apricot-sweep-2</a></strong> to <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/sweeps/amzq1zqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3osw89lt' target=\"_blank\">https://wandb.ai/necs_lab/SpikeVisionSeedDBv1/runs/3osw89lt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n",
      "The directory [data/DVS128/integrate_fixed_15000_events] already exists.\n",
      "Random Seed: 30\n",
      "Train Loader Length: 74\n",
      "Test Loader Length: 19\n",
      "Epoch [1/250], Train Loss: 2.3979, Train Accuracy: 8.11%, Test Loss: 0.1555, Test Accuracy: 8.87%\n",
      "Epoch [2/250], Train Loss: 2.3979, Train Accuracy: 8.11%, Test Loss: 0.1485, Test Accuracy: 15.02%\n",
      "Epoch [3/250], Train Loss: 2.0092, Train Accuracy: 31.51%, Test Loss: 0.1280, Test Accuracy: 48.12%\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    # Initialize wandb\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Set default hyperparameters if not provided in sweep config\n",
    "        if not hasattr(config, 'num_epochs'):\n",
    "            config.num_epochs = 250\n",
    "        if not hasattr(config, 'batch_size'):\n",
    "            config.batch_size = 20\n",
    "        if not hasattr(config, 'learning_rate'):\n",
    "            config.learning_rate = 1e-3\n",
    "        if not hasattr(config, 'embed_dims'):\n",
    "            config.embed_dims = 256\n",
    "        if not hasattr(config, 'time_steps'):\n",
    "            config.time_steps = 16\n",
    "        if not hasattr(config, 'patch_size'):\n",
    "            config.patch_size = 8\n",
    "        if not hasattr(config, 'input_extend'):\n",
    "            config.input_extend = 1\n",
    "        if not hasattr(config, 'thresholds'):\n",
    "            config.thresholds = [128/128, 128/128, 128/128]\n",
    "        if not hasattr(config, 'pooling_stat'):\n",
    "            config.pooling_stat = \"1111\"\n",
    "        if not hasattr(config, 'mlp_ratio'):\n",
    "            config.mlp_ratio = 2\n",
    "        if not hasattr(config, 'num_heads'):\n",
    "            config.num_heads = 1\n",
    "        if not hasattr(config, 'layers'):\n",
    "            config.layers = 2\n",
    "        if not hasattr(config, 'train_threshold'):\n",
    "            config.train_threshold = False\n",
    "        if not hasattr(config, 'image_size'):\n",
    "            config.image_size = 128\n",
    "        if not hasattr(config, 'image_size_w'):\n",
    "            config.image_size_w = 128\n",
    "        if not hasattr(config, 'in_channels'):\n",
    "            config.in_channels = 2\n",
    "        if not hasattr(config, 'num_classes'):\n",
    "            config.num_classes = 11\n",
    "        if not hasattr(config, 'manual_seed'):\n",
    "            config.manual_seed = 49\n",
    "\n",
    "        # Data Preparation\n",
    "        def binarize(tensor):\n",
    "            return (tensor > 0.5).float()\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            binarize\n",
    "        ])\n",
    "\n",
    "        # Combine and split dataset\n",
    "        combined_dataset = ConcatDataset([\n",
    "            DVS128Gesture(\n",
    "                \"data/DVS128/\",\n",
    "                train=True,\n",
    "                data_type=\"frame\",\n",
    "                custom_integrate_function=integrate_fixed_15000_events\n",
    "            ),\n",
    "            DVS128Gesture(\n",
    "                \"data/DVS128/\",\n",
    "                train=False,\n",
    "                data_type=\"frame\",\n",
    "                custom_integrate_function=integrate_fixed_15000_events\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        train_size = int(0.8 * len(combined_dataset))\n",
    "        test_size = len(combined_dataset) - train_size\n",
    "        \n",
    "        # Use a deterministic generator with a manual seed\n",
    "        generator = torch.Generator()\n",
    "        seed = np.random.randint(low=0, high=100)\n",
    "        print(f\"Random Seed: {seed}\")\n",
    "        generator.manual_seed(config.manual_seed)\n",
    "        \n",
    "        dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(\n",
    "            dataset_train,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            dataset_test,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loader Length: {len(train_loader)}\")\n",
    "        print(f\"Test Loader Length: {len(test_loader)}\")\n",
    "    \n",
    "\n",
    "        # Model instantiation\n",
    "        model = SpikeVisionModel(\n",
    "            T=config.time_steps,\n",
    "            img_size_h=config.image_size,\n",
    "            img_size_w=config.image_size_w,\n",
    "            in_channels=config.in_channels,\n",
    "            num_classes=config.num_classes,\n",
    "            patch_size=config.patch_size,\n",
    "            embed_dims=config.embed_dims,\n",
    "            num_heads=config.num_heads,\n",
    "            pooling_stat=config.pooling_stat,\n",
    "            mlp_ratio=config.mlp_ratio,\n",
    "            threshold_head=config.thresholds[0],\n",
    "            threshold_sps=config.thresholds[1],\n",
    "            threshold_enc=config.thresholds[2],\n",
    "            depths=config.layers,\n",
    "            train_threshold=config.train_threshold       \n",
    "        )\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        # Training Loop\n",
    "        num_epochs = config.num_epochs\n",
    "        best_accuracy = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # TRAIN\n",
    "            model.train()\n",
    "            train_loss_sum = 0.0\n",
    "            train_correct = 0\n",
    "            total_train_samples = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)  # Move images to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                # Reshape images for spiking classifier\n",
    "                images = images.squeeze(1)  # Remove channel dimension\n",
    "                #images = images.permute(1, 0, 2, 3, 4)  # Convert [B, T, C, H, W] to [T, B, C, H, W]\n",
    "\n",
    "                # Forward pass\n",
    "                outputs, _, _ = model(images)  # Outputs: [B, num_classes]\n",
    "                #print(f\"Logits shape: {outputs.shape}, Labels shape: {labels.shape}\")\n",
    "                #print(f\"Labels: {labels}\")\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Metrics\n",
    "                train_loss_sum += loss.item() * labels.size(0)\n",
    "                train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                total_train_samples += labels.size(0)\n",
    "\n",
    "            train_loss = train_loss_sum / total_train_samples\n",
    "            train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs, _, _ = model(images)  # Extract only logits\n",
    "                    test_loss += criterion(outputs, labels).item()\n",
    "                    _, predicted = outputs.max(1)  # Get predicted classes\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            test_loss = test_loss / total\n",
    "            test_accuracy = 100. * correct / total\n",
    "\n",
    "            # Log metrics and hyperparameters to wandb\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"learning_rate\": config.learning_rate,\n",
    "                \"batch_size\": config.batch_size,\n",
    "                \"time_steps\": config.time_steps,\n",
    "                \"embed_dims\": config.embed_dims,\n",
    "                \"patch_size\": config.patch_size,\n",
    "                \"input_extend\": config.input_extend,\n",
    "                \"num_epoch\": config.num_epochs,\n",
    "                \"thresholds\": config.thresholds,\n",
    "                \"pooling_stat\": config.pooling_stat,\n",
    "                \"mlp_ratio\": config.mlp_ratio,\n",
    "                \"num_heads\": config.num_heads,\n",
    "                \"layers\": config.layers,\n",
    "                \"train_threshold\": config.train_threshold,\n",
    "                \"manual_seed\": config.manual_seed,\n",
    "            })\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "                  f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "            # Stop if accuracy at epoch 20 is below 84%\n",
    "            if epoch + 1 == 20 and test_accuracy < 73.0:\n",
    "                print(f\"Test accuracy below 73% at epoch 20. Stopping current training run.\")\n",
    "                return  # Ends current `train` function, letting wandb agent start a new run.\n",
    "\n",
    "            # Save best model\n",
    "            if (test_accuracy > best_accuracy) and (test_accuracy > 88):\n",
    "                best_accuracy = test_accuracy\n",
    "                # Include configuration parameters in the filename\n",
    "                model_save_path = (\n",
    "                    f\"savedmodels/tsdt_model_epoch{epoch+1}_acc{test_accuracy:.2f}_\"\n",
    "                    f\"lr{config.learning_rate}_bs{config.batch_size}_tsteps{config.time_steps}_\"\n",
    "                    f\"embed{config.embed_dims}_patch{config.patch_size}_ext{config.input_extend}_\"\n",
    "                    f\"seeddb{config.manual_seed}_seeddb_\"\n",
    "                    f\"layers{config.layers}_heads{config.num_heads}_mlp{config.mlp_ratio}.pth\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                wandb.save(model_save_path)\n",
    "                print(f'New best model saved with test accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "        print(f'Training complete. Best test accuracy: {best_accuracy:.2f}%')\n",
    "\n",
    "# Start the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"SpikeVisionSeedDBv1\")\n",
    "wandb.agent(sweep_id, function=train, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Using model: {model_type}, sps module: {spsmodule}, encoder module: {encodermodule}. Seed: {seed}\")\n",
    "# model.half()\n",
    "# ipdb.set_trace()\n",
    "#load=0\n",
    "#if load:\n",
    "#    precision_epochs = 10\n",
    "#train(\n",
    "#    model, \n",
    "#    loader_train, \n",
    "#    loader_test, \n",
    "#    optimizer, \n",
    "#    scheduler, \n",
    "#    num_epochs=num_epoch,\n",
    "#    chan=in_channels,\n",
    "#    precision_epochs=precision_epochs,\n",
    "#    load=load,\n",
    "#    path=path,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "# plt.plot(train_eval_loss_list, label='Train eval Loss')\n",
    "plt.plot(test_loss_list, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_low = folder +\"dataset_\" + str(data_index+1) + \"_timestep_\"+str(time_steps)+\"_head_\"+str(num_heads)+\"_mlp_ratio_\"+str(mlp_ratio)+\"_encoder_\"+str(layers)+\"_embed_dims_\"+str(embed_dims)+\"_nbits_\"+str(n_bits)+\"_ep_\"+str(num_epoch)+\"_lr(e-3)_\"+str(learning_rate * 1000)+ \"_low_precision\"+\".pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "base_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load('DVS128best99_65_2layer_8.pth')\n",
    "# state_dict = torch.load('DVS128best97_95_2layer.pth')\n",
    "state_dict = torch.load(\"result/DVS128/DVS128best98_29_2layer.pth\")\n",
    "#state_dict = torch.load(path_low)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)\n",
    "test_acc, train_eval_acc, test_loss = test(model, loader_train, loader_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeds = []\n",
    "test_accs = []\n",
    "def custom_collate_fn(batch):\n",
    "    max_timesteps = 8 \n",
    "    padded_batch = []\n",
    "\n",
    "    for item in batch:\n",
    "        data, label = item\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data).float()\n",
    "\n",
    "        current_timesteps = data.size(0)\n",
    "\n",
    "        if current_timesteps < max_timesteps:\n",
    "            padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "            padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "        else:\n",
    "            padded_data = data[:max_timesteps]\n",
    "\n",
    "        padded_batch.append((padded_data, label))\n",
    "\n",
    "    return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "for seed in tqdm(range(100)):\n",
    "    generator.manual_seed(seed)\n",
    "    dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)\n",
    "    \n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    test_acc, train_eval_acc, test_loss = test(model, loader_train, loader_test)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "for i in range(100):\n",
    "    print(i, test_accs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_accs)):\n",
    "    if test_accs[i] == 0.9829351535836177:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "step_accs = []\n",
    "generator.manual_seed(49)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)\n",
    "# loader_train = DataLoader(\n",
    "#     dataset_train,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=custom_collate_fn,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=False,\n",
    "# )\n",
    "# loader_test = DataLoader(\n",
    "#     dataset_test,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=custom_collate_fn,\n",
    "#     num_workers=num_workers,\n",
    "#     pin_memory=False,\n",
    "# )\n",
    "dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)\n",
    "for max_timesteps in tqdm(range(1, 32)):\n",
    "    def custom_collate_fn(batch):\n",
    "    \n",
    "        padded_batch = []\n",
    "    \n",
    "        for item in batch:\n",
    "            data, label = item\n",
    "            if isinstance(data, np.ndarray):\n",
    "                data = torch.from_numpy(data).float()\n",
    "    \n",
    "            current_timesteps = data.size(0)\n",
    "    \n",
    "            if current_timesteps < max_timesteps:\n",
    "                padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "                padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "            else:\n",
    "                padded_data = data[:max_timesteps]\n",
    "    \n",
    "            padded_batch.append((padded_data, label))\n",
    "    \n",
    "        return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "    \n",
    "    \n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    test_acc, train_eval_acc, test_loss = test(model, loader_train, loader_test)\n",
    "    step_accs.append(test_acc)\n",
    "for i in range(1, 32):\n",
    "    print(f\"Time step amount: {i}, Test accuracy: {step_accs[i - 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.plot(range(1, 27), step_accs[0:26], 'r', marker='o')\n",
    "plt.xlabel('Time steps', fontsize=25, fontweight='heavy')\n",
    "plt.ylabel('Accuracy', fontsize=25, fontweight='heavy')\n",
    "plt.xticks(fontsize=18, fontweight='heavy')\n",
    "plt.yticks(fontsize=18, fontweight='heavy')\n",
    "\n",
    "# for i, v in enumerate(step_accs):\n",
    "#     # plt.vlines(i+1, 0, v, linestyles='dashed', colors='gray')\n",
    "    \n",
    "#     plt.text(i+1, v, f'{v:.4f}', ha='center', va='bottom', fontsize=20)\n",
    "previous_y = None \n",
    "for i, v in enumerate(step_accs):\n",
    "    if i < 26:\n",
    "        height = v\n",
    "        if previous_y is not None and abs(previous_y - height) < 0.01:\n",
    "            height = v + 0.01\n",
    "    \n",
    "        plt.text(i + 1, height, f'{v:.3f}', ha='center', va='bottom', fontsize=15)\n",
    "        previous_y = height\n",
    "plt.ylim(0.7, 1.02)\n",
    "plt.savefig('time_step_acc.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_order = []\n",
    "\n",
    "for max_timesteps in tqdm(range(1, 3)):\n",
    "    def custom_collate_fn(batch):\n",
    "    \n",
    "        padded_batch = []\n",
    "    \n",
    "        for item in batch:\n",
    "            data, label = item\n",
    "            if isinstance(data, np.ndarray):\n",
    "                data = torch.from_numpy(data).float()\n",
    "    \n",
    "            current_timesteps = data.size(0)\n",
    "    \n",
    "            if current_timesteps < max_timesteps:\n",
    "                padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "                padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "            else:\n",
    "                padded_data = data[:max_timesteps]\n",
    "    \n",
    "            padded_batch.append((padded_data, label))\n",
    "    \n",
    "        return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "    \n",
    "    \n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    example = enumerate(loader_test)\n",
    "    _, (tensors, labels) = next(example)\n",
    "    arrays_order.append(tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(arrays_order[0][0] == arrays_order[1][0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for images, labels in loader_test:\n",
    "        outputs, _, _ = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.numpy())  \n",
    "        y_pred.extend(predicted.cpu().numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat_normalized, annot=True, fmt='.2f', cmap='Reds', xticklabels=list(range(11)), yticklabels=list(range(11)))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for DVS128')\n",
    "plt.savefig('confusion_matrix_DVS128.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'DVS128best95_90_2layer_seed'+str(seed)+'.pth')\n",
    "torch.save(model.state_dict(), f'DVS128_smaller_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_path = folder + f\"c_implementation/\"\n",
    "c_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 2**n_bits\n",
    "print(threshold)\n",
    "ss_dict = model.state_dict()\n",
    "for name, param in model.state_dict().items():\n",
    "    param = (param*threshold).data.cpu().numpy()\n",
    "    param = np.round(param)\n",
    "    ss_dict[i] = torch.tensor(param)\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "    print(param, \"\\n\")  \n",
    "    weights_1d = param.flatten()\n",
    "    \n",
    "\n",
    "    np.savetxt(c_path + f\"{name}_weights.txt\", weights_1d, fmt=\"%d\")\n",
    "    \n",
    "    # 打印确认\n",
    "    print(f\"Layer: {name} | Saved shape: {weights_1d.shape}\")\n",
    "# ss_dict = model.state_dict()\n",
    "# for i in ss_dict.keys():\n",
    "#     key = (ss_dict[i]*threshold).data.cpu().numpy()\n",
    "#     key = np.round(key)\n",
    "#     ss_dict[i] = torch.tensor(key)#, dtype = torch.int)\n",
    "    \n",
    "print(ss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    weights_1d = param.view(-1).cpu().numpy()  # 转换为一维数组\n",
    "    \n",
    "    # 保存到文件，每一层的权重保存为独立的文件\n",
    "    np.savetxt(f\"{name}_weights.txt\", weights_1d, fmt=\"%.6f\")\n",
    "    \n",
    "    # 打印确认\n",
    "    print(f\"Layer: {name} | Saved shape: {weights_1d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(c_path + 'input_data.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "data_array = np.array([list(map(float, line.split())) for line in data])\n",
    "reshaped_arr = data_array.reshape(20, 8, 2, 128, 128)\n",
    "transposed_arr = reshaped_arr.transpose(0, 1, 3, 4, 2)\n",
    "final_arr = transposed_arr.reshape(20, 8, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_final_arr = final_arr.reshape(160*128, 256)\n",
    "np.savetxt(c_path + 'buffer_output.txt', reshaped_final_arr, fmt='%d', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
