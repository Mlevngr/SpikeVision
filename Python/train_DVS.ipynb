{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federico/aienv/aienv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Authors: z.shen1@tue.nl, f.corradi@tue.nl\n",
    "# Training SpikeVision for DVS 128 dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from timm.scheduler.step_lr import StepLRScheduler\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/federico/aienv/aienv/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "folder_src = \"./results/\"\n",
    "try:\n",
    "    os.mkdir(folder_src)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dataset = \"DVS128\"\n",
    "folder = folder_src + f\"{dataset}/\"\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from SV import SpikeVision\n",
    "\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from torch.amp import GradScaler, autocast\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "layers = 2\n",
    "in_channels = 2\n",
    "train_threshold = False\n",
    "image_size = 128\n",
    "dataset_classes = 11\n",
    "num_epochs = 200\n",
    "time_steps = 8\n",
    "embed_dim = 256\n",
    "threshold = [128/128, 128/128, 128/128]\n",
    "pooling_state = \"1111\"\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "precision_epochs = num_epochs\n",
    "precision_bits = 8\n",
    "load = False\n",
    "train_loss_fn, test_loss_fn = criteria, criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating\n",
    "event_per_map = 15000\n",
    "def integrate_fixed_events(events, H, W, events_per_map = event_per_map):\n",
    "    t, x, y, p = (events[key] for key in ('t', 'x', 'y', 'p'))\n",
    "    total_events = len(t)\n",
    "    num_maps = int(np.ceil(total_events / events_per_map))\n",
    "    frames = np.zeros([num_maps, 2, H, W], dtype=np.float32)\n",
    "\n",
    "    for i in range(num_maps):\n",
    "        start_index = i * events_per_map\n",
    "        end_index = min((i + 1) * events_per_map, total_events)\n",
    "        for j in range(start_index, end_index):\n",
    "            if p[j] == 1:\n",
    "                frames[i, 1, y[j], x[j]] += 1\n",
    "            else:\n",
    "                frames[i, 0, y[j], x[j]] += 1\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../data/DVS128/integrate_fixed_events] already exists.\n",
      "The directory [../data/DVS128/integrate_fixed_events] already exists.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = DVS128Gesture(\n",
    "    \"../data/DVS128/\",\n",
    "    train=True,\n",
    "    data_type=\"frame\",\n",
    "    custom_integrate_function=integrate_fixed_events\n",
    ")\n",
    "dataset_test = DVS128Gesture(\n",
    "    \"../data/DVS128/\",\n",
    "    train=False,\n",
    "    data_type=\"frame\", \n",
    "    custom_integrate_function=integrate_fixed_events\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "combined_dataset = ConcatDataset([dataset_train, dataset_test])\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "test_size = len(combined_dataset) - train_size\n",
    "generator = torch.Generator()\n",
    "seed = np.random.randint(low=0, high=100)\n",
    "print(seed)\n",
    "generator.manual_seed(seed)\n",
    "dataset_train, dataset_test = random_split(combined_dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "def custom_collate_fn(batch):\n",
    "    max_timesteps = time_steps\n",
    "    padded_batch = []\n",
    "\n",
    "    for item in batch:\n",
    "        data, label = item\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = torch.from_numpy(data).float()\n",
    "\n",
    "        current_timesteps = data.size(0)\n",
    "\n",
    "        if current_timesteps < max_timesteps:\n",
    "            padding_size = (0, 0, 0, 0, 0, 0, 0, max_timesteps - current_timesteps)\n",
    "            padded_data = torch.nn.functional.pad(data, pad=padding_size, mode='constant', value=0)\n",
    "        else:\n",
    "            padded_data = data[:max_timesteps]\n",
    "\n",
    "        padded_batch.append((padded_data, label))\n",
    "\n",
    "    return torch.utils.data.dataloader.default_collate(padded_batch)\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=8,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = SpikeVision(\n",
    "    dataset=dataset,\n",
    "    image_size_h=image_size,\n",
    "    image_size_w=image_size,\n",
    "    input_channels=in_channels,\n",
    "    num_classes=dataset_classes,\n",
    "    embed_dims=embed_dim,\n",
    "    threshold_head=threshold[0],\n",
    "    threshold_conv=threshold[1],\n",
    "    threshold_scre=threshold[2],\n",
    "    depths=layers,\n",
    "    pooling_state=pooling_state,\n",
    "    train_threshold=train_threshold,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_eval_loss_list = []\n",
    "test_loss_list = []\n",
    "def train(model, train_loader, test_loader, optimizer, scheduler=None, num_epochs=50, precision_epochs=num_epochs, low_precision=8, load=False, path=None):\n",
    "    if load:\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    acc_state = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_acc = 0\n",
    "        train_loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        predictions = []\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f\"{len(train_loader)} batches in one epoch.\")\n",
    "        \n",
    "        for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "\n",
    "            with autocast(device_type=f\"{device}\", dtype=torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = train_loss_fn(outputs, labels)\n",
    "                prediction = outputs.argmax(axis=1)\n",
    "            scaler.scale(loss).backward()\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "            train_acc += (prediction == labels).sum().item()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        if (epoch >= precision_epochs):\n",
    "            low_precision_state = low_precision(model.state_dict(), precision=low_precision)\n",
    "            if epoch == precision_epochs:\n",
    "                torch.save(model.state_dict(), path)\n",
    "                path = folder + \"low_precision_model\" + path\n",
    "                for param in optimizer.param_groups:\n",
    "                    param['lr'] = 1e-3\n",
    "            model.load_state_dict(low_precision_state)\n",
    "            model.to(device)\n",
    "        \n",
    "        test_acc, train_eval_acc, test_loss = test(model, test_loader, train_loader)\n",
    "        if test_acc >= acc_state:\n",
    "            acc_state = test_acc\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(\"Checkpoint saved.\")\n",
    "        \n",
    "        train_loss = train_loss_sum / len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        scheduler.step(epoch)\n",
    "\n",
    "        print(f\"Highest test accuracy: {acc_state}\")\n",
    "        print(f\"Epoch: {epoch:3d}, Train loss: {train_loss:.4f}, Train accuracy: {train_acc / len(train_loader.dataset):.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.eval()\n",
    "        if len(images.shape) == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        outputs = model(images)\n",
    "        loss = test_loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        prediction = outputs.argmax(axis=1)\n",
    "        total_correct += (prediction == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = total_correct / len(loader.dataset)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "def test(model, test_loader, train_loader):\n",
    "    model.eval()\n",
    "    test_acc, test_loss = evaluate(model, test_loader)\n",
    "    train_eval_acc, train_eval_loss = evaluate(model, train_loader)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Train eval Loss: {train_eval_loss:.4f}, Train eval Acc: {train_eval_acc:.4f}')\n",
    "    return test_acc, train_eval_acc, test_loss\n",
    "\n",
    "def precision_transfer(x, precision_bits=8, threshold=1.0, max_value=0.5, min_value=-0.5):\n",
    "    x_flat = x.flatten()\n",
    "    step = np.diff(np.linspace(min_value, max_value, num=2**precision_bits)[0:2])\n",
    "    max_value = max_value - step[0]\n",
    "    q_list = np.round(np.linspace(min_value, max_value, num=2**precision_bits), precision_bits - 2)\n",
    "\n",
    "    func = lambda x: q_list[np.abs(q_list - x).argmin()]\n",
    "    q_list = np.array(list(map(func, x_flat)))\n",
    "    q_list = q_list.reshape(x.shape)\n",
    "    return q_list\n",
    "\n",
    "def low_precision(state_dict, precision=8, threshold=1.0, max_value=0.5, min_value=-0.5):\n",
    "    for key in state_dict.keys():\n",
    "        if \"threshold\" in key:\n",
    "            continue\n",
    "        state_dict[key] = precision_transfer(state_dict[key].cpu().numpy(), precision_bits=precision, threshold=threshold)\n",
    "\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler(args, optimizer):\n",
    "    num_epochs = args.epochs\n",
    "\n",
    "    if getattr(args, 'lr_noise', None) is not None:\n",
    "        lr_noise = getattr(args, 'lr_noise')\n",
    "        if isinstance(lr_noise, (list, tuple)):\n",
    "            noise_range = [n * num_epochs for n in lr_noise]\n",
    "            if len(noise_range) == 1:\n",
    "                noise_range = noise_range[0]\n",
    "        else:\n",
    "            noise_range = lr_noise * num_epochs\n",
    "    else:\n",
    "        noise_range = None\n",
    "    noise_args = dict(\n",
    "        noise_range_t=noise_range,\n",
    "        noise_pct=getattr(args, 'lr_noise_pct', 0.67),\n",
    "        noise_std=getattr(args, 'lr_noise_std', 1.),\n",
    "        noise_seed=getattr(args, 'seed', 42),\n",
    "    )\n",
    "    cycle_args = dict(\n",
    "        cycle_mul=getattr(args, 'lr_cycle_mul', 1.),\n",
    "        cycle_decay=getattr(args, 'lr_cycle_decay', 0.1),\n",
    "        cycle_limit=getattr(args, 'lr_cycle_limit', 1),\n",
    "    )\n",
    "    lr_scheduler = StepLRScheduler(\n",
    "        optimizer,\n",
    "        decay_t=args.decay_epochs,\n",
    "        decay_rate=args.decay_rate,\n",
    "        warmup_lr_init=args.warmup_lr,\n",
    "        warmup_t=args.warmup_epochs,\n",
    "        **noise_args,\n",
    "    )\n",
    "\n",
    "\n",
    "    return lr_scheduler, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "learning_rate = 1e-3\n",
    "path = folder + f\"model_{dataset}_embeddim_{embed_dim}_depth_{layers}_{time}.pth\"\n",
    "cooldown = 10\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epochs = num_epochs\n",
    "        self.sched = 'step'  \n",
    "        self.min_lr = 1e-5\n",
    "        self.warmup_lr = 3e-4\n",
    "        self.warmup_epochs = 20\n",
    "        self.decay_rate = 0.9\n",
    "        self.cooldown_epochs = cooldown\n",
    "        self.lr_noise = [0.6, 0.9]\n",
    "        self.lr_noise_pct = 0.67\n",
    "        self.lr_noise_std = 1.0\n",
    "        self.seed = 42\n",
    "        self.decay_epochs = 20\n",
    "        self.patience_epochs = 5\n",
    "\n",
    "args = Args()\n",
    "\n",
    "optimizer = torch.optim.Adam([{ 'params': base_params}], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "scheduler, num_epochs = create_scheduler(args, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                       | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 batches in one epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|                                                                                                                                                                                                       | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision_bits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, scheduler, num_epochs, precision_epochs, low_precision, load, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_loss_fn(outputs, labels)\n\u001b[1;32m     26\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tue/jupyter/SpikeVision/Python/SV.py:92\u001b[0m, in \u001b[0;36mSpikeVision.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (this_t)\u001b[38;5;241m/\u001b[39mT\n\u001b[0;32m---> 92\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mthis_t\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m     94\u001b[0m     screener_out \u001b[38;5;241m=\u001b[39m blk(conv_out)\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tue/jupyter/SpikeVision/Python/conv.py:87\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x, threshold)\u001b[0m\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif_input(x, threshold\u001b[38;5;241m=\u001b[39mthreshold)\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.reshape(T, B, -1, H // ratio, W // ratio).contiguous()\u001b[39;00m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif_conv1(x)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_stat[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aienv/aienv/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "train(model, loader_train, loader_test, optimizer, scheduler, num_epochs=num_epochs, precision_epochs=precision_epochs, low_precision=precision_bits, load=load, path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
